{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Эксперименты с моделью\n",
    "\n",
    "На прошлой неделе вы поучаствовали в соревновании на kaggle и, наверняка, большинство успешно справилось с прохождением baseline, а значит пора двигаться дальше - заняться оптимизацией модели, провести серию экспериментов и построить сильное финальное решения.\n",
    "\n",
    "В этом задании вам нужно провести ряд эскпериментов, оценить качество полученных в процессе экспериментирования моделей и выбрать лучшее решение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет оцениваться на основании загруженного jupyther notebook и развернутых ответов на поставленные вопросы.\n",
    "\n",
    "## Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, recall_score, precision_score, log_loss, make_scorer\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "seed = 1903\n",
    "first_categorial_index = 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявим функции, необходимые для построения базовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numericna(train_frame, test_frame, averageCalculator):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме значениями, посчитанными averageCalculator. \"\"\"\n",
    "    \n",
    "    # Посчитаем средние по колонкам\n",
    "    numeric_avgs = averageCalculator(train_frame)\n",
    "    \n",
    "    # Оставим только те колонки, в которых среднее значение не равно NaN, т.к. в таких колонках совсем нет значений\n",
    "    numeric_avgs = numeric_avgs.dropna()\n",
    "    dropped_columns = train_frame.columns.drop(numeric_avgs.index)\n",
    "    n_frame_train = train_frame[list(numeric_avgs.index)]\n",
    "    n_frame_test = test_frame[list(numeric_avgs.index)]\n",
    "    \n",
    "    # Заполним пропущенные численные значения средними\n",
    "    n_frame_train = n_frame_train.fillna(numeric_avgs, axis=0)\n",
    "    n_frame_test = n_frame_test.fillna(numeric_avgs, axis=0)\n",
    "    return (n_frame_train, n_frame_test, dropped_columns)\n",
    "\n",
    "def fill_numericna_means(train_frame, test_frame):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме средними и удаляет те колонки, в которых значений нет. \"\"\"\n",
    "    return fill_numericna(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        lambda f: f.mean(axis=0, skipna=True))\n",
    "\n",
    "def fill_categorial_nav(train_frame, test_frame):\n",
    "    return train_frame.fillna(\"NaV\"), test_frame.fillna(\"NaV\")\n",
    "\n",
    "def remove_constant_features(frame, min_count=2):\n",
    "    \"\"\"Функция удаляет колонки, которые содержат только одно значение.\"\"\"\n",
    "    \n",
    "    # Посчитаем количества уникальных значений по колонкам\n",
    "    unique_counts = frame.nunique()\n",
    "    # Удалим колонки с количеством значений меньшим min_count\n",
    "    columns_to_drop = unique_counts[unique_counts < min_count].index\n",
    "    \n",
    "    return (frame.drop(columns=columns_to_drop), columns_to_drop)\n",
    "\n",
    "class MatrixLabelEncoder:\n",
    "    \"\"\" Класс кодирует категории числами от 0 до n, где n количество категорий в колонке. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoders = []\n",
    "    \n",
    "    def fit(self, matrix):\n",
    "        for column_number in range(matrix.shape[1]):\n",
    "            column = matrix[:,column_number]\n",
    "            labelEncoder = LabelEncoder().fit(column)\n",
    "            self.encoders.append(labelEncoder)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, matrix):\n",
    "        transformed = np.empty(matrix.shape)\n",
    "        for column_number in range(matrix.shape[1]):\n",
    "            labelEncoder = self.encoders[column_number]\n",
    "            num_column = labelEncoder.transform(matrix[:,column_number])\n",
    "            for row_number, val in enumerate(num_column):\n",
    "                transformed[row_number, column_number] = val\n",
    "        return transformed\n",
    "    \n",
    "class CompositeEncoder:\n",
    "    \"\"\" Класс принимает набор фабрик с методами fit и transform и последовательно применяет их \"\"\"\n",
    "    def __init__(self, encoder_factories):\n",
    "        self.encoder_factories = encoder_factories\n",
    "        \n",
    "    def fit(self, matrix):\n",
    "        encoders = []\n",
    "        transformed = matrix\n",
    "        for encoder_factory in self.encoder_factories:\n",
    "            encoder = encoder_factory().fit(transformed)\n",
    "            encoders.append(encoder)\n",
    "            transformed = encoder.transform(transformed)\n",
    "        self.encoders = encoders\n",
    "        return self\n",
    "\n",
    "    def transform(self, matrix):\n",
    "        for encoder in self.encoders:\n",
    "            matrix = encoder.transform(matrix)\n",
    "        return matrix\n",
    "    \n",
    "def predict_ridge_proba(X, model):\n",
    "    \"\"\" Функция возвращает вероятности предсказаний для класса churn модель Ridge \"\"\"\n",
    "    # Поскольку RidgeClassifier не обладает функцией predict_proba приходится считать его вручную\n",
    "    # Подробнее можно посмотреть здесь:\n",
    "    # https://www.codesd.com/item/scikit-learn-ridge-classifier-extract-class-probabilities.html\n",
    "    func = model.decision_function(X)\n",
    "    return np.exp(func) / (1 + np.exp(func))\n",
    "\n",
    "def predict_model_proba(X, model):\n",
    "    \"\"\" Функция возвращает вероятности предсказаний для класса churn \"\"\"\n",
    "    return list(zip(*model.predict_proba(X)))[1]\n",
    "    \n",
    "def stratifiedKFold_fscore(\n",
    "    frame,\n",
    "    labels,\n",
    "    model_factory,\n",
    "    process_frame,\n",
    "    frame_to_matrix,\n",
    "    numeric_features,\n",
    "    categorial_features,\n",
    "    predict_probabilities,\n",
    "    seed,\n",
    "    folds_count = 3):\n",
    "    \"\"\" Функция разбивает набор данных на folds_count, считает ROC-AUC на каждом фолде\n",
    "        и возвращает усредненное по фолдам значение.\n",
    "        Функция также возвращает модель, показавшую лучшее качество, её метрики и разделение данных.\n",
    "        Разделение данных нужно для того, чтобы строить метрики модели на данных, на которых она не обучалась.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=folds_count, shuffle=True, random_state=seed)\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_table = None\n",
    "    best_split = None\n",
    "    best_encoders = None\n",
    "    best_dropped_columns = None\n",
    "    metrics_sum = 0\n",
    "    for train_indices, test_indices in skf.split(frame, labels):\n",
    "        # Разобьем фрем на train и test с помощью функции process_frame\n",
    "        # Внутри такой функции мы можем по-разному обрабатывать признаки обучаясь только на train наборе.\n",
    "        train_frame, train_labels, test_frame, test_labels, dropped_numeric, dropped_categorial = process_frame(\n",
    "            frame.loc[train_indices, :],\n",
    "            labels.loc[train_indices, :],\n",
    "            frame.loc[test_indices, :],\n",
    "            labels.loc[test_indices, :],\n",
    "            numeric_features,\n",
    "            categorial_features)\n",
    "        numeric_cleaned = numeric_features.drop(dropped_numeric)\n",
    "        categorial_cleaned = categorial_features.drop(dropped_categorial)\n",
    "        # Преобразуем фреймы в матрицы.\n",
    "        # Тут можно выполнить финальное преобразование признаков, например масштабирование признаков.\n",
    "        # В функции frame_to_matrix энкодеры типа StandardScaler обучаются только на train признаках.\n",
    "        X_train, X_test, y_train, y_test, num_encoder, cat_encoder = frame_to_matrix(\n",
    "            train_frame,\n",
    "            test_frame,\n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            numeric_cleaned,\n",
    "            categorial_cleaned)\n",
    "\n",
    "        model = model_factory()\n",
    "        # Обучим модель\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Построим вероятности принадлежности к целевому классу\n",
    "        probabilities = predict_probabilities(X_test, model)\n",
    "        # Считаем roc auc score\n",
    "        rocAuc = roc_auc_score(y_test, probabilities)\n",
    "        metrics_sum += rocAuc\n",
    "        if(best_model is None or best_score < rocAuc):\n",
    "            # В случае, если модель лучше предыдущих сохраним её\n",
    "            # оценку, модель, матрицу ошибок и разделение данных\n",
    "            best_score = rocAuc\n",
    "            best_model = model\n",
    "            best_dropped_columns = (dropped_numeric, dropped_categorial)\n",
    "            best_encoders = (num_encoder, cat_encoder)\n",
    "            best_split = (train_frame, train_labels, test_frame, test_labels)\n",
    "    return (\n",
    "        metrics_sum/folds_count,\n",
    "        best_model,\n",
    "        best_score,\n",
    "        best_split,\n",
    "        best_encoders,\n",
    "        best_dropped_columns)\n",
    "\n",
    "def cleanup_frame_common(frame, numeric_features, categorial_features):\n",
    "    \"\"\"Функция делит признакина числовые и категориальные и удаляет константные признаки, содержащие только одно значение\"\"\"\n",
    "    # Разделим коллекции на группы - числовые и категориальные.\n",
    "    numeric_frame = frame[numeric_features].copy()\n",
    "    categorial_frame = frame[categorial_features].copy()\n",
    "    # Удалим вещественные колонки, содержащие одно и менее значений. 0 значений мы получаем, когда значения во всех строках Nan.\n",
    "    numeric_frame_no_const, dropped_const_numeric_columns = remove_constant_features(numeric_frame)\n",
    "    \n",
    "    # Удалим категориальные колонки, содержащие ноль значений. Если есть одно значение, то могут быть Nan, которые для\n",
    "    # категориальных признаков могут быть еще одной категорией (зависит от стратегии обработки).\n",
    "    categorial_frame_no_const, dropped_const_categorial_columns = remove_constant_features(categorial_frame, 1)\n",
    "    \n",
    "    # Восстановим фрейм и вернем вместе с ним список удаленных категориальных колонок.\n",
    "    return (pd.concat([numeric_frame, categorial_frame], axis=1),\n",
    "            list(dropped_const_numeric_columns),\n",
    "            list(dropped_const_categorial_columns))\n",
    "\n",
    "def process_frame(\n",
    "    train_frame,\n",
    "    train_labels,\n",
    "    test_frame,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features,\n",
    "    fill_na_numerics,\n",
    "    fill_na_categorial):\n",
    "    \"\"\" Функция обрабатывает числовые признаки, заполняя пропуски. \"\"\"\n",
    "    \n",
    "    # Удалим константные колонки из train_frame, и такие-же колонки из test_frame\n",
    "    train_frame, const_numeric_columns, const_categorial_columns = cleanup_frame_common(\n",
    "        train_frame,\n",
    "        numeric_features,\n",
    "        categorial_features)\n",
    "    test_frame = test_frame.drop(columns=const_numeric_columns)\n",
    "    test_frame = test_frame.drop(columns=const_categorial_columns)\n",
    "    \n",
    "    numeric_features = numeric_features.drop(const_numeric_columns)\n",
    "    categorial_features = categorial_features.drop(const_categorial_columns)\n",
    "    \n",
    "    # Заполним пропущенные вещественные значения\n",
    "    numeric_train, numeric_test, dropped_numeric = fill_na_numerics(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    numeric_features = numeric_features.drop(dropped_numeric)\n",
    "    \n",
    "    # Заполним пропущенные категориальные значения строками \"NaV\" (Not a value)\n",
    "    categorial_train, categorial_test = fill_na_categorial(train_frame[categorial_features], test_frame[categorial_features])\n",
    "    \n",
    "    # Удалим категориальные колонки с одним единственным значением\n",
    "    categorial_train, dropped_categorial = remove_constant_features(categorial_train)\n",
    "    categorial_test = categorial_test.drop(columns=dropped_categorial)\n",
    "    \n",
    "    categorial_features = categorial_features.drop(dropped_categorial)\n",
    "    \n",
    "    # Список удаленных колонок\n",
    "    dropped_numeric = np.concatenate([\n",
    "        list(const_numeric_columns),\n",
    "        list(dropped_numeric)])\n",
    "    dropped_categorial = np.concatenate([\n",
    "        list(const_categorial_columns),\n",
    "        list(dropped_categorial)])\n",
    "    \n",
    "    return (pd.concat([numeric_train, categorial_train], axis=1),\n",
    "            train_labels,\n",
    "            pd.concat([numeric_test, categorial_test], axis=1),\n",
    "            test_labels,\n",
    "            dropped_numeric,\n",
    "            dropped_categorial)\n",
    "\n",
    "def process_frame_base(\n",
    "    train_frame,\n",
    "    train_labels,\n",
    "    test_frame,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features):\n",
    "    return process_frame(\n",
    "        train_frame,\n",
    "        train_labels,\n",
    "        test_frame,\n",
    "        test_labels,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        fill_numericna_means,\n",
    "        fill_categorial_nav)\n",
    "\n",
    "def scale_features(train_frame, test_frame):\n",
    "    train_numeric = train_frame.as_matrix()\n",
    "    \n",
    "    scaler = StandardScaler().fit(train_numeric)\n",
    "    \n",
    "    train_numeric = coo_matrix(scaler.transform(train_numeric))\n",
    "    test_numeric = coo_matrix(scaler.transform(test_frame.as_matrix()))\n",
    "    \n",
    "    return (train_numeric, test_numeric, scaler)\n",
    "\n",
    "def one_hot_features(train_frame, test_frame):\n",
    "    fit_matrix = pd.concat([train_frame, test_frame]).as_matrix()\n",
    "    \n",
    "    if fit_matrix.shape[0] == 0 or fit_matrix.shape[1] == 0:\n",
    "        return (coo_matrix(train_frame.as_matrix()), coo_matrix(test_frame.as_matrix()), None)\n",
    "    categorial_encoder = CompositeEncoder([MatrixLabelEncoder, OneHotEncoder]).fit(fit_matrix)\n",
    "    \n",
    "    train_categorial = categorial_encoder.transform(train_frame.as_matrix())\n",
    "    test_categorial = categorial_encoder.transform(test_frame.as_matrix())\n",
    "    \n",
    "    return (train_categorial, test_categorial, categorial_encoder)\n",
    "\n",
    "def int_label_features(train_frame, test_frame):\n",
    "    fit_matrix = pd.concat([train_frame, test_frame]).as_matrix()\n",
    "    categorial_encoder = MatrixLabelEncoder().fit(fit_matrix)\n",
    "    \n",
    "    train_categorial = categorial_encoder.transform(train_frame.as_matrix())\n",
    "    test_categorial = categorial_encoder.transform(test_frame.as_matrix())\n",
    "    \n",
    "    return (train_categorial, test_categorial, categorial_encoder)\n",
    "\n",
    "def frame_to_matrix_one_hot(\n",
    "    train_frame,\n",
    "    test_frame,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features):\n",
    "    \"\"\" Функци преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные с помощью OneHotEncoding. \"\"\"\n",
    "    # Масштабируем вещественные признаки\n",
    "    train_numeric, test_numeric, scaler = scale_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    # One hot encode для категориальных признаков\n",
    "    train_categorial, test_categorial, categorial_encoder = one_hot_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    y_train = train_labels.as_matrix().flatten()\n",
    "    y_test = test_labels.as_matrix().flatten()\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            y_train,\n",
    "            y_test,\n",
    "            scaler,\n",
    "            categorial_encoder)\n",
    "\n",
    "def frame_to_matrix_labeled(\n",
    "    train_frame,\n",
    "    test_frame,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features):\n",
    "    \"\"\" Функция преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные целыми числами. \"\"\"\n",
    "    \n",
    "    # Масштабируем вещественные признаки\n",
    "    train_numeric, test_numeric, scaler = scale_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    train_categorial, test_categorial, categorial_encoder = int_label_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    y_train = train_labels.as_matrix().flatten()\n",
    "    y_test = test_labels.as_matrix().flatten()\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            y_train,\n",
    "            y_test,\n",
    "            scaler,\n",
    "            categorial_encoder)\n",
    "\n",
    "def ridge_baseline_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        process_frame_base,\n",
    "        frame_to_matrix_one_hot,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_ridge_proba,\n",
    "        seed)\n",
    "\n",
    "def random_forest_baseline_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RandomForestClassifier,\n",
    "        process_frame_base,\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def gradient_boosting_baseline_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        process_frame_base,\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27999, 230)\n",
      "(27999, 1)\n"
     ]
    }
   ],
   "source": [
    "churn_data_frame = pd.read_csv(\"..\\..\\Data\\churn_data_train.csv\", \",\", dtype= { \"Var73\": np.float64 })\n",
    "churn_labels_frame = pd.read_csv(\"..\\..\\Data\\churn_labels_train.csv\", dtype= { \"labels\": np.int64 })\n",
    "print(churn_data_frame.shape)\n",
    "print(churn_labels_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим числовые и категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = churn_data_frame.columns[:first_categorial_index]\n",
    "categorial_columns = churn_data_frame.columns[first_categorial_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовые модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_base = ridge_baseline_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "random_forest_base = random_forest_baseline_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "gradient_boosting_base = gradient_boosting_baseline_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На 2-й неделе я выбрал в качестве основной метрики F-Score, после 4-й неделе я решил изменить метрику. В качестве основной метрики я буду использовать ROC-AUC. Причина проста. Я максимизирую площадь под ROC кривой, а потом с помощью того-же F-Score могу подобрать оптимальный порог, чтобы максимизировать качество предсказаний.\n",
    "\n",
    "## Инструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Начнем с простого. Давайте оценим как много объектов действительно нужно для построения качественной модели. Для обучения доступна достаточно большая выборка и может так оказаться, что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. Постройте кривые обучения, обучая модель на выборках разного размера начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. Обратите внимание на `sklearn.model_selection.learning_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_frame(frame, labels):\n",
    "    \"\"\" Функция перемешивает фрейм \"\"\"\n",
    "    y_name = \"labels\"\n",
    "    frame[y_name] = labels[y_name]\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    labels = pd.DataFrame(frame[y_name], columns=[y_name])\n",
    "    frame = frame.drop(columns=[y_name])\n",
    "    return frame, labels\n",
    "\n",
    "def half_frame(frame, labels):\n",
    "    \"\"\" Функция разделяет фрейм на два пополам \"\"\"\n",
    "    half_index = int(frame.shape[0]/2)\n",
    "    first_half = frame.loc[:half_index, :].reset_index(drop=True)\n",
    "    first_half_labels = labels.loc[:half_index, :].reset_index(drop=True)\n",
    "    second_half = frame.loc[half_index+1:, :].reset_index(drop=True)\n",
    "    second_half_labels = labels.loc[half_index+1:, :].reset_index(drop=True)\n",
    "    \n",
    "    first_half, first_half_labels = shuffle_frame(first_half, first_half_labels)\n",
    "    second_half, second_half_labels = shuffle_frame(second_half, second_half_labels)\n",
    "    return (first_half, first_half_labels, second_half, second_half_labels)\n",
    "\n",
    "def find_best_frame_size(\n",
    "    pinned_data,\n",
    "    data,\n",
    "    best_metric,\n",
    "    best_size,\n",
    "    round_size=5):\n",
    "    \"\"\" Функция находит размер фрейма, дающий наилучшее качество. \"\"\"\n",
    "    step_metric = best_metric\n",
    "    step_frame, step_labels = data\n",
    "    left_frame = None\n",
    "    better_metric = best_metric\n",
    "    better_size = best_size\n",
    "    \n",
    "    while step_metric >= best_metric:\n",
    "        step_frame, step_labels, left_frame, left_labels = half_frame(step_frame, step_labels)\n",
    "        \n",
    "        pinned_frame,pinned_labels = pinned_data\n",
    "        step_frame = pd.concat([pinned_frame, step_frame], axis=0, ignore_index=True)\n",
    "        step_labels = pd.concat([pinned_labels, step_labels], axis=0, ignore_index=True).astype(np.int64)\n",
    "        step_model = gradient_boosting_baseline_builder(\n",
    "            step_frame,\n",
    "            step_labels,\n",
    "            numeric_columns,\n",
    "            categorial_columns)\n",
    "        step_metric = np.round(step_model[0], round_size)\n",
    "        if(step_metric >= better_metric):\n",
    "            better_metric = step_metric\n",
    "            better_size = step_frame.shape\n",
    "        print (\"Frame_size %i: %.5f\\tInitial quality: %.5f\\tLeft_size: %i\" % (step_frame.shape[0], step_metric, best_metric, left_frame.shape[0]))\n",
    "    \n",
    "    if(left_frame.shape[0] < 14):\n",
    "        return (better_metric, better_size)\n",
    "    else:\n",
    "        return find_best_frame_size(\n",
    "            (step_frame, step_labels),\n",
    "            (left_frame, left_labels),\n",
    "            better_metric,\n",
    "            better_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame_size 14000: 0.69898\tInitial quality: 0.73199\tLeft_size: 13999\n",
      "Frame_size 21000: 0.71753\tInitial quality: 0.73199\tLeft_size: 6999\n",
      "Frame_size 24500: 0.72973\tInitial quality: 0.73199\tLeft_size: 3499\n",
      "Frame_size 26250: 0.72812\tInitial quality: 0.73199\tLeft_size: 1749\n",
      "Frame_size 27125: 0.73019\tInitial quality: 0.73199\tLeft_size: 874\n",
      "Frame_size 27563: 0.72796\tInitial quality: 0.73199\tLeft_size: 436\n",
      "Frame_size 27782: 0.72974\tInitial quality: 0.73199\tLeft_size: 217\n",
      "Frame_size 27891: 0.72876\tInitial quality: 0.73199\tLeft_size: 108\n",
      "Frame_size 27946: 0.73180\tInitial quality: 0.73199\tLeft_size: 53\n",
      "Frame_size 27973: 0.72954\tInitial quality: 0.73199\tLeft_size: 26\n",
      "Frame_size 27987: 0.72853\tInitial quality: 0.73199\tLeft_size: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.73199000000000003, (27999, 230))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_frame_size(\n",
    "    (pd.DataFrame([], columns=churn_data_frame.columns), pd.DataFrame([], columns=churn_labels_frame.columns)),\n",
    "    (churn_data_frame, churn_labels_frame),\n",
    "    np.round(gradient_boosting_base[0], 5),\n",
    "    churn_data_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Удалим константные колонки\n",
    "curve_frame, const_numeric_columns, const_categorial_columns = cleanup_frame_common(\n",
    "    churn_data_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "\n",
    "curve_num_features = numeric_columns.drop(const_numeric_columns)\n",
    "curve_cat_features = categorial_columns.drop(const_categorial_columns)\n",
    "\n",
    "# Посчитаем средние по колонкам\n",
    "numeric_curve_frame = curve_frame[curve_num_features]\n",
    "numeric_avgs = numeric_curve_frame.mean(axis=0, skipna=True)\n",
    "\n",
    "# Оставим только те колонки, в которых среднее значение не равно NaN, т.к. в таких колонках совсем нет значений\n",
    "numeric_avgs = numeric_avgs.dropna()\n",
    "dropped_numeric = numeric_curve_frame.columns.drop(numeric_avgs.index)\n",
    "numeric_curve_frame = numeric_curve_frame[list(numeric_avgs.index)]\n",
    "\n",
    "# Заполним пропущенные численные значения средними\n",
    "numeric_curve_frame = numeric_curve_frame.fillna(numeric_avgs, axis=0)\n",
    "curve_num_features = curve_num_features.drop(dropped_numeric)\n",
    "\n",
    "# Заполним пропущенные категориальные значения строками \"NaV\" (Not a value)\n",
    "cat_curve_frame = curve_frame[curve_cat_features].fillna(\"NaV\")\n",
    "\n",
    "# Удалим категориальные колонки с одним единственным значением\n",
    "cat_curve_frame, dropped_categorial = remove_constant_features(cat_curve_frame)\n",
    "curve_cat_features = curve_cat_features.drop(dropped_categorial)\n",
    "\n",
    "curve_cleaned_frame = pd.concat([numeric_curve_frame, cat_curve_frame], axis=1)\n",
    "\n",
    "# Масштабируем вещественные признаки\n",
    "scaler = StandardScaler().fit(numeric_curve_frame)\n",
    "curve_num_matrix = coo_matrix(scaler.transform(numeric_curve_frame))\n",
    "\n",
    "# Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "categorial_encoder = MatrixLabelEncoder().fit(cat_curve_frame.as_matrix())\n",
    "curve_cat_matrix = categorial_encoder.transform(cat_curve_frame.as_matrix())\n",
    "\n",
    "curve_x = hstack([curve_num_matrix, curve_cat_matrix])\n",
    "curve_y = churn_labels_frame.as_matrix().flatten()\n",
    "\n",
    "item_counts, train_scores, test_scores = learning_curve(\n",
    "    GradientBoostingClassifier(),\n",
    "    curve_x,\n",
    "    curve_y,\n",
    "    scoring=lambda e, x, y: roc_auc_score(y, list(zip(*e.predict_proba(x)))[1]),\n",
    "    shuffle=True,\n",
    "    train_sizes=[0.2,0.4,0.6,0.8,1],\n",
    "    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAHwCAYAAACYMcj+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253163dd898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(12,8))\n",
    "for idx, count in enumerate(item_counts):\n",
    "        train_score_curve = train_scores[idx]\n",
    "        test_score_curve = test_scores[idx]\n",
    "        plt.plot(\n",
    "            test_score_curve,\n",
    "            label=count)\n",
    "plt.legend()\n",
    "plt.ylabel(\"ROC AUC score\")\n",
    "plt.xlabel(\"Experiment #\")\n",
    "plt.title(\"Test curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По графикам видно, что если выбросить из выборки 20% сэмплов, то качество понижается несущественно. Однако если выбросить больше, то качество уже заметно снижается. На мой взгляд не получится существенно снизить скорость обучения модели за счет уменьшения количества сэмплов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Часто несбалансированные по классам выборки приводят к различным проблемам при обучении моделей. Давайте попробуем по-разному обработать выборку, поиграть с распределением объектов по классам и сделать выводы о том, как соотношение классов влияет на качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1\\. Задайте веса объектам так, чтобы соотношение классов с учетом весов объектов изменилось. Попробуйте не менее трёх различных вариантов весов. Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_Gradient_Boosting:\n",
    "    \"\"\" Класс-обертка для GradientBoostingClassifier. Задает веса классов в конструкторе, а не методе fit. \n",
    "        В методе fit преобразует веса классов в веса сэмплов, так как этого требует GradientBoostingClassifier.\"\"\"\n",
    "    def __init__(self, class_weights=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.model = GradientBoostingClassifier()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        sample_weight = None\n",
    "        if not(self.class_weights is None):\n",
    "            sample_weight = []\n",
    "            for class_value in y:\n",
    "                weight = self.class_weights[class_value]\n",
    "                sample_weight.append(weight)\n",
    "        self.model = self.model.fit(X, y, sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "def gradient_boosting_weighted_builder(frame, labels, numeric_features, categorial_features, class_weights=None):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        lambda: Weighted_Gradient_Boosting(class_weights),\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: process_frame(tf, tl, tsf, tsl, nf, cf, fill_numericna_means, fill_categorial_nav),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x1bc01fc6808>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_weights = [1, 3, 6, 9, 12]\n",
    "weighted_models = []\n",
    "\n",
    "for churn_weight in churn_weights:\n",
    "    weighted_models.append(gradient_boosting_weighted_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    {-1: 1, 1: churn_weight})[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.73240857212958144),\n",
       " (3, 0.73181997855368452),\n",
       " (6, 0.72725180949066548),\n",
       " (9, 0.72651626109880107),\n",
       " (12, 0.72191783865633197)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(churn_weights, weighted_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.72202087916107105, 0.72040660516688082)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_weights_1_12[0], gb_weights_12_1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравновешивание моделей за счет весов не дало прироста качества модели. Даже наоборот качество несколько упало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.2\\. Примените к выборке технологию undersampling: для этого нужно убрать из обучения некоторое количество объектов большего класса таким образом, чтобы соотношение классов изменилось. Попробуйте не менее трёх различных вариантов undersampling (варианты могут отличаться как по количество отфильтрованных объектов, так и по принципу выборка объектов для отсеивания из выборки). Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(frame, labels, classes, take_items_count, seed, balance=1, y_name=\"labels\"):\n",
    "    min_class = None\n",
    "    min_size = labels.shape[0]\n",
    "    selected_items = {}\n",
    "    for frame_class in classes:\n",
    "        class_only_labels = labels[labels[y_name] == frame_class]\n",
    "        selected_items[frame_class] = class_only_labels\n",
    "        class_size = class_only_labels.shape[0]\n",
    "        if class_size < min_size:\n",
    "            min_size = class_size\n",
    "            min_class = frame_class\n",
    "    if min_class is None:\n",
    "        return frame, labels\n",
    "    else:\n",
    "        selection_count = int(min_size*balance)\n",
    "        undersampled_frame = pd.DataFrame([], columns=frame.columns)\n",
    "        undersampled_labels = pd.DataFrame([], columns=labels.columns)\n",
    "        for frame_class in classes:\n",
    "            class_only_labels = selected_items[frame_class]\n",
    "            class_indices = list(class_only_labels.index)\n",
    "            class_only_frame = frame.loc[class_indices,:]\n",
    "            \n",
    "            if frame_class != min_class:\n",
    "                class_only_frame[y_name] = class_only_labels[y_name]\n",
    "                class_only_frame = take_items_count(\n",
    "                    class_only_frame.reset_index(drop=True),\n",
    "                    selection_count,\n",
    "                    seed)\n",
    "                \n",
    "                class_only_labels = pd.DataFrame(class_only_frame[y_name], columns=[y_name])\n",
    "                class_only_frame = class_only_frame.drop(columns=[y_name])\n",
    "            undersampled_frame = pd.concat([undersampled_frame, class_only_frame], ignore_index=True)\n",
    "            undersampled_labels = pd.concat([undersampled_labels, class_only_labels], ignore_index=True).astype(np.int64)\n",
    "                \n",
    "        return undersampled_frame, undersampled_labels\n",
    "    \n",
    "def take_first_items_count(frame, count, seed):\n",
    "    return frame.head(count).reset_index(drop=True)\n",
    "\n",
    "def take_items_random(frame, count, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    rnd_indices = np.random.randint(0, frame.shape[0], size=count)\n",
    "    return frame.loc[rnd_indices, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_frame_sampling(frame_to_balance, labels, repeat_times):\n",
    "    \"\"\" Функция балансирует соотношения классов сэмплируя класс churn. \"\"\"\n",
    "    y_name = \"labels\"\n",
    "    churn_only_labels = labels[labels[y_name] == 1]\n",
    "    churn_indices = list(churn_only_labels.index)\n",
    "    churn_only_frame = frame_to_balance.loc[churn_indices,:]\n",
    "    churn_balanced_frame = frame_to_balance.copy()\n",
    "    churn_balanced_labels = labels.copy()\n",
    "    for i in range(0, (repeat_times-1)):\n",
    "        churn_balanced_frame = pd.concat([churn_balanced_frame, churn_only_frame], ignore_index=True)\n",
    "        churn_balanced_labels = pd.concat([churn_balanced_labels, churn_only_labels], ignore_index=True)\n",
    "        \n",
    "    churn_balanced_frame[y_name] = churn_balanced_labels[y_name]\n",
    "    churn_balanced_frame = churn_balanced_frame.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    churn_balanced_labels = pd.DataFrame(churn_balanced_frame[y_name], columns=[y_name]).astype(np.int64)\n",
    "    churn_balanced_frame = churn_balanced_frame.drop(columns=[y_name])\n",
    "    \n",
    "    return (churn_balanced_frame, churn_balanced_labels)\n",
    "\n",
    "def balance_train_frame(train_frame, train_labels, test_frame, test_labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция балансирует train frame, после этого обрабатывает признаки с помощью метода\n",
    "        process_frame_base_model \"\"\"\n",
    "    balanced_train, balanced_labels = balance_frame_sampling(train_frame, train_labels, repeat_times)\n",
    "    return process_frame(\n",
    "        balanced_train,\n",
    "        balanced_labels,\n",
    "        test_frame,\n",
    "        test_labels,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        fill_numericna_means,\n",
    "        fill_categorial_nav)\n",
    "    \n",
    "def gb_oversampled_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_undersampling(\n",
    "    train_frame,\n",
    "    train_labels,\n",
    "    test_frame,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features,\n",
    "    items_selector,\n",
    "    balance):\n",
    "    \"\"\" Функция балансирует train frame, после этого обрабатывает признаки с помощью метода process_frame_base_model \"\"\"\n",
    "    balanced_train, balanced_labels = undersample(\n",
    "        train_frame,\n",
    "        train_labels,\n",
    "        [-1, 1],\n",
    "        items_selector,\n",
    "        seed)\n",
    "\n",
    "    return process_frame(\n",
    "        balanced_train,\n",
    "        balanced_labels,\n",
    "        test_frame,\n",
    "        test_labels,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        fill_numericna_means,\n",
    "        fill_categorial_nav)\n",
    "\n",
    "def gb_first_items_undersampled_builder(frame, labels, numeric_features, categorial_features, balance):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_undersampling(tf, tl, tsf, tsl, nf, cf, take_first_items_count, balance),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def gb_random_undersampled_builder(frame, labels, numeric_features, categorial_features, balance):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_undersampling(tf, tl, tsf, tsl, nf, cf, take_items_random, balance),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def ridge_first_items_undersampled_builder(frame, labels, numeric_features, categorial_features, balance):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_undersampling(tf, tl, tsf, tsl, nf, cf, take_first_items_count, balance),\n",
    "        frame_to_matrix_one_hot,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_ridge_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampling_balances = [1, 2, 3]\n",
    "gb_undersampled_first_items_models = []\n",
    "gb_undersampled_random_models = []\n",
    "ridge_undersampled_first_items_models = []\n",
    "\n",
    "for balance in undersampling_balances:\n",
    "    gb_undersampled_first_items_models.append(gb_first_items_undersampled_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        balance))\n",
    "    gb_undersampled_random_models.append(gb_random_undersampled_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        balance))\n",
    "    ridge_undersampled_first_items_models.append(ridge_first_items_undersampled_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gradient boosting sequential  Gradient boosting random  Ridge sequential\n",
      "1                      0.712794                  0.714268          0.649256\n",
      "2                      0.712370                  0.714268          0.649256\n",
      "3                      0.712370                  0.714268          0.649256\n"
     ]
    }
   ],
   "source": [
    "comparison_columns = [\"Gradient boosting sequential\", \"Gradient boosting random\", \"Ridge sequential\"]\n",
    "comparison_data = []\n",
    "for idx, balance in enumerate(undersampling_balances):\n",
    "    comparison_data.append([\n",
    "        gb_undersampled_first_items_models[idx][0],\n",
    "        gb_undersampled_random_models[idx][0],\n",
    "        ridge_undersampled_first_items_models[idx][0]])\n",
    "print(pd.DataFrame(comparison_data, index=undersampling_balances, columns=comparison_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72325453536248041"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_oversampled = gb_oversampled_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73199183318256378, 0.72325453536248041)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boosting_base[0], gb_oversampled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравновешивание моделей с помощью undersampling и oversampling дало несколько нежиданный результат.\n",
    "Видно, что при undersampling качество модели падает больше чем на процент. Думаю это может быть связано с тем, что выборка при выбрасывании моделей очень сильно уменьшается (до 2778 сэмплов), из-за чего и страдает качество.\n",
    "\n",
    "При oversampling качество тоже несколько снижается.\n",
    "\n",
    "В принципе, если понижение качества на один процент не играет роли, то можно уменьшить время обучения модели за счет undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Теперь перейдем к работе с признаками. Ранее вы реализовали несколько стратегий для обработки пропущенных значений. Сравните эти стратегии между собой с помощью оценки качества моделей кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка пропущенных значений сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numericna_medians(train_frame, test_frame):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме медианами и удаляет те колонки, в которых значений нет. \"\"\"\n",
    "    return fill_numericna(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        lambda f: f.median(axis=0, skipna=True))\n",
    "\n",
    "def fill_numericna_modes(train_frame, test_frame):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме модами и удаляет те колонки, в которых значений нет. \"\"\"\n",
    "    return fill_numericna(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        lambda f: f.mode(axis=0).iloc[0,:])\n",
    "\n",
    "def gb_numeric_medians_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: process_frame(tf, tl, tsf, tsl, nf, cf, fill_numericna_medians, fill_categorial_nav),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def gb_numeric_modes_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: process_frame(tf, tl, tsf, tsl, nf, cf, fill_numericna_modes, fill_categorial_nav),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним пропуски в вещественных признаках медианами и модами и сравним полученное качество с базовой моделью, где пропуски заполнены средними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7234480234826256, 0.72414063842056386, 0.73199183318256378)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_numeric_medians = gb_numeric_medians_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "gb_numeric_modes = gb_numeric_modes_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "gb_numeric_medians[0], gb_numeric_modes[0], gradient_boosting_base[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За счет изменения стратегии обработки пропусков в вещественных признаках качество повысить не удалось. Заполнение пропусков средними по колонке выглядит оптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_frequent_values(train_frame, test_frame):\n",
    "    \"\"\" Функция заполняет пустые значения самым частым значением в колонке. \"\"\"\n",
    "    train_result = train_frame.copy()\n",
    "    test_result = test_frame.copy()\n",
    "    for column in train_result.columns:\n",
    "        frequencies = train_result[column].value_counts()\n",
    "        if (len(frequencies) < 1):\n",
    "            continue\n",
    "        most_frequent_value = frequencies.index[0]\n",
    "        train_result[column] = train_result[column].fillna(most_frequent_value)\n",
    "        test_result[column] = test_result[column].fillna(most_frequent_value)\n",
    "    return train_result, test_result\n",
    "\n",
    "def gb_categorial_frequent_values_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: process_frame(tf, tl, tsf, tsl, nf, cf, fill_numericna_means, fill_na_frequent_values),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним пропуски в категориальных признаках самым частым значением признака в колонке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.72703473329445079, 0.73199183318256378)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_categorial_frequent_values = gb_categorial_frequent_values_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "gb_categorial_frequent_values[0], gradient_boosting_base[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повысить качество за счет заполнения категориальных признаков частотными значениями не удалось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_features(train_frame, test_frame):\n",
    "    fit_matrix = pd.concat([train_frame, test_frame]).T.to_dict().values()\n",
    "    categorial_encoder = DictVectorizer().fit(fit_matrix)\n",
    "    \n",
    "    train_categorial = categorial_encoder.transform(train_frame.T.to_dict().values())\n",
    "    test_categorial = categorial_encoder.transform(test_frame.T.to_dict().values())\n",
    "    \n",
    "    return (train_categorial, test_categorial, categorial_encoder)\n",
    "\n",
    "def frame_to_matrix_dict(\n",
    "    train_frame,\n",
    "    test_frame,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features):\n",
    "    \"\"\" Функци преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные с помощью OneHotEncoding. \"\"\"\n",
    "    \n",
    "    # Масштабируем вещественные признаки\n",
    "    train_numeric, test_numeric, scaler = scale_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    # One hot encode для категориальных признаков\n",
    "    train_categorial, test_categorial, categorial_encoder = dict_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    y_train = train_labels.as_matrix().flatten()\n",
    "    y_test = test_labels.as_matrix().flatten()\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            y_train,\n",
    "            y_test,\n",
    "            scaler,\n",
    "            categorial_encoder)\n",
    "\n",
    "def ridge_dict_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: process_frame(tf, tl, tsf, tsl, nf, cf, fill_numericna_means, fill_categorial_nav),\n",
    "        frame_to_matrix_dict,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_ridge_proba,\n",
    "        seed)\n",
    "\n",
    "def gb_dict_builder(frame, labels, numeric_features, categorial_features):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: process_frame(tf, tl, tsf, tsl, nf, cf, fill_numericna_means, fill_categorial_nav),\n",
    "        frame_to_matrix_dict,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_ridge_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем заменить OneHotEncoder на DictVectorizer и посмотрим как это скажется на качестве. Поскольку OneHotEncoder увеличивает качество линейной модели, то построим с DictVectorizer в т.ч. линейную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.67512496899847863,\n",
       " 0.57375604497316912,\n",
       " 0.73199183318256378,\n",
       " 0.6893237897545238)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_dict = ridge_dict_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "gb_dict = gb_dict_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "ridge_base[0], ridge_dict[0], gradient_boosting_base[0], gb_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого я применил следующие алгоритмы обработки категориальных признаков:\n",
    "    - заполнение пропусков самым частым значением\n",
    "    - заполнение пропусков новым значением (NaV)\n",
    "    - преобразование признаков с помощью DictVectorizer\n",
    "    - преобразование признаков с помощью OneHotEncoder\n",
    "    - преобразование признаков с помощью LabelEncoder\n",
    "\n",
    "По результатам видно, что использование DictVectorizer вместо OneHotEncoder для линейной модели уменьшает качество аж на 10%. Использование DictVectorizer вместо LabelEncoder для GradientBoosting понижает качество на 4.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлой неделе я использовал алгоритм add-del для отбора признаков. Сравним качество, полученное на признаках, отобранных с помощью этого алгоритма с другими подходами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73143177904640944,\n",
       " 0.73234680588912349,\n",
       " 0.747949546705029,\n",
       " 0.74273775549762355)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_del_num_features = [\n",
    "    \"Var90\", \"Var99\", \"Var110\", \"Var113\", \"Var116\", \"Var126\", \"Var127\",\n",
    "    \"Var140\", \"Var142\", \"Var147\", \"Var157\", \"Var161\", \"Var172\", \"Var186\",\n",
    "    \"Var187\", \"Var189\", \"Var190\", \"Var3\", \"Var4\", \"Var5\", \"Var7\", \"Var13\",\n",
    "    \"Var14\", \"Var16\", \"Var17\", \"Var25\", \"Var28\", \"Var29\", \"Var34\", \"Var49\",\n",
    "    \"Var51\", \"Var53\", \"Var58\", \"Var61\", \"Var73\", \"Var81\", \"Var82\", \"Var83\"]\n",
    "add_del_cat_features = [\n",
    "    \"Var221\", \"Var229\", \"Var191\", \"Var193\", \"Var194\", \"Var196\", \"Var201\",\n",
    "    \"Var205\", \"Var208\", \"Var211\", \"Var212\", \"Var213\", \"Var215\", \"Var217\",\n",
    "    \"Var218\"]\n",
    "add_del_features = add_del_num_features + add_del_cat_features\n",
    "\n",
    "gb_add_del = gradient_boosting_baseline_builder(\n",
    "    churn_data_frame[add_del_features],\n",
    "    churn_labels_frame,\n",
    "    pd.Index(add_del_num_features),\n",
    "    pd.Index(add_del_cat_features))\n",
    "gb_add_del[0], gradient_boosting_base[0], gb_add_del[2], gradient_boosting_base[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что среднее качество полученное на алгоритме add-del не повысилось. Зато существенно увеличилась скорость обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем для отбора признаков SelectKBest из библиотеки sklearn.feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "def frame_to_matrix_kbest(\n",
    "    train_frame,\n",
    "    test_frame,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features,\n",
    "    k):\n",
    "    train_matrix, test_matrix, train_labels_mtx, test_labels_mtx, sc, enc = frame_to_matrix_labeled(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        train_labels,\n",
    "        test_labels,\n",
    "        numeric_features,\n",
    "        categorial_features)\n",
    "    selector = SelectKBest(k=k).fit(train_matrix, train_labels_mtx)\n",
    "    train_matrix = selector.transform(train_matrix)\n",
    "    test_matrix = selector.transform(test_matrix)\n",
    "    \n",
    "    return (train_matrix,\n",
    "            test_matrix,\n",
    "            train_labels_mtx,\n",
    "            test_labels_mtx,\n",
    "            sc,\n",
    "            enc)\n",
    "\n",
    "def gradient_boosting_kbest_builder(frame, labels, numeric_features, categorial_features, k):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        process_frame_base,\n",
    "        lambda tf, tsf, tl, tsl, nf, cf: frame_to_matrix_kbest(tf, tsf, tl, tsl, nf, cf, k),\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73208237572347412, 0.73234680588912349)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_kbest = gradient_boosting_kbest_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    60)\n",
    "gb_kbest[0], gradient_boosting_base[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем для отбора признаков SelectFdr из библиотеки sklearn.feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFdr\n",
    "def frame_to_matrix_fdr(\n",
    "    train_frame,\n",
    "    test_frame,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    numeric_features,\n",
    "    categorial_features,\n",
    "    alpha):\n",
    "    train_matrix, test_matrix, train_labels_mtx, test_labels_mtx, sc, enc = frame_to_matrix_labeled(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        train_labels,\n",
    "        test_labels,\n",
    "        numeric_features,\n",
    "        categorial_features)\n",
    "    selector = SelectFdr(alpha=alpha).fit(train_matrix, train_labels_mtx)\n",
    "    train_matrix = selector.transform(train_matrix)\n",
    "    test_matrix = selector.transform(test_matrix)\n",
    "    \n",
    "    return (train_matrix,\n",
    "            test_matrix,\n",
    "            train_labels_mtx,\n",
    "            test_labels_mtx,\n",
    "            sc,\n",
    "            enc)\n",
    "\n",
    "def gradient_boosting_fdr_builder(frame, labels, numeric_features, categorial_features, alpha=0.05):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        process_frame_base,\n",
    "        lambda tf, tsf, tl, tsl, nf, cf: frame_to_matrix_fdr(tf, tsf, tl, tsl, nf, cf, alpha),\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73256213966789918, 0.73234680588912349)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_fdr = gradient_boosting_fdr_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "gb_fdr[0], gradient_boosting_base[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 features k-best:  0.731918842375 0.732346805889\n",
      "40 features k-best:  0.732501224569 0.732346805889\n",
      "21 features k-best:  0.735627198952 0.732346805889\n"
     ]
    }
   ],
   "source": [
    "for count in [60, 40, 21]:\n",
    "    gb_kbest = gradient_boosting_kbest_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        count)\n",
    "    print(str(count) + \" features k-best: \", gb_kbest[0], gradient_boosting_base[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимальным количеством фич для алгоритма k-best является 21 (Тут надо признать, что я конечно провел больше 3х экспериментов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 features FDR:  0.734424444575 0.732346805889\n",
      "0.02 features FDR:  0.734815478137 0.732346805889\n",
      "0.025 features FDR:  0.734876244788 0.732346805889\n",
      "0.03 features FDR:  0.734390064761 0.732346805889\n",
      "0.035 features FDR:  0.733480041925 0.732346805889\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.01, 0.02, 0.025, 0.03, 0.035]:\n",
    "    gb_fdr = gradient_boosting_fdr_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        alpha)\n",
    "    print(str(alpha) + \" features FDR: \", gb_fdr[0], gradient_boosting_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27999, 21), (27999,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_frame, preprocessed_labels, pf, pl, dropped_num_cols, dropped_cat_cols = process_frame_base(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns)\n",
    "\n",
    "clear_num_columns = numeric_columns.drop(dropped_num_cols)\n",
    "clear_cat_columns = categorial_columns.drop(dropped_cat_cols)\n",
    "\n",
    "preprocessed_x, px, preprocessed_y, py, ne, ce = frame_to_matrix_kbest(\n",
    "    preprocessed_frame,\n",
    "    pf,\n",
    "    preprocessed_labels,\n",
    "    pl,\n",
    "    clear_num_columns,\n",
    "    clear_cat_columns,\n",
    "    21)\n",
    "\n",
    "preprocessed_x.shape, preprocessed_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.2, loss='deviance', max_depth=5,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "               presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "               warm_start=False), 0.51664933925003875)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "tune_params = {\n",
    "    \"loss\": [\"deviance\", \"exponential\"],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"n_estimators\": [80, 100, 120],\n",
    "    \"max_depth\": [2,3,5]\n",
    "}\n",
    "\n",
    "estimation_result = GridSearchCV(GradientBoostingClassifier(), tune_params, scoring=roc_auc_scorer).fit(\n",
    "    preprocessed_x,\n",
    "    preprocessed_y)\n",
    "estimation_result.best_estimator_, estimation_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_params_builder(frame, labels, numeric_features, categorial_features, k, params):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        lambda: GradientBoostingClassifier(learning_rate=params[\"learning_rate\"], n_estimators=params[\"n_estimators\"], max_depth=params[\"max_depth\"]),\n",
    "        process_frame_base,\n",
    "        lambda tf, tsf, tl, tsl, nf, cf: frame_to_matrix_kbest(tf, tsf, tl, tsl, nf, cf, k),\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.72013867039234913, 0.73550269049261063, 0.73199183318256378)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params_1 = gradient_boosting_params_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    21,\n",
    "    {\n",
    "        \"learning_rate\": 0.2,\n",
    "        \"n_estimators\": 80,\n",
    "        \"max_depth\": 5\n",
    "    })\n",
    "gb_params_1[0], gb_kbest[0], gradient_boosting_base[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='exponential', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_params_2 = {\n",
    "    \"loss\": [\"exponential\"],\n",
    "    \"n_estimators\": [120, 140, 160, 180],\n",
    "    \"max_depth\": [5, 7, 9, 11]\n",
    "}\n",
    "\n",
    "estimation_result_2 = GridSearchCV(GradientBoostingClassifier(), tune_params_2).fit(preprocessed_x, preprocessed_y)\n",
    "estimation_result_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73020055429043784, 0.73550269049261063, 0.73199183318256378)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params_2 = gradient_boosting_params_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    21,\n",
    "    {\n",
    "        \"loss\": \"deviance\",\n",
    "        \"n_estimators\": 160,\n",
    "        \"max_depth\": 5\n",
    "    })\n",
    "gb_params_2[0], gb_kbest[0], gradient_boosting_base[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повысить качество за счет подбора параметров GradientBoostingClassifier не удалось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Var189', 168.53999834060247),\n",
       " ('Var7', 138.72396029560818),\n",
       " ('Var218', 137.2227289436527),\n",
       " ('Var73', 133.55011699962719),\n",
       " ('Var207', 102.89224301476428),\n",
       " ('Var229', 82.801907052937594),\n",
       " ('Var113', 81.849727248593794),\n",
       " ('Var144', 80.90708401606885),\n",
       " ('Var13', 63.811817425539232),\n",
       " ('Var126', 54.184525747264786),\n",
       " ('Var65', 44.848639710782159),\n",
       " ('Var193', 38.408317957057513),\n",
       " ('Var211', 31.447816346853354),\n",
       " ('Var74', 30.508489203834287),\n",
       " ('Var81', 28.423406450109585),\n",
       " ('Var205', 27.934883013676473),\n",
       " ('Var228', 27.210062827759188),\n",
       " ('Var210', 24.40728674047406),\n",
       " ('Var72', 21.712533257764612),\n",
       " ('Var227', 17.613599825910271),\n",
       " ('Var216', 17.597994397105545)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_x, px, l_y, py, ne, ce = frame_to_matrix_labeled(\n",
    "    preprocessed_frame,\n",
    "    pf,\n",
    "    preprocessed_labels,\n",
    "    pl,\n",
    "    clear_num_columns,\n",
    "    clear_cat_columns)\n",
    "\n",
    "mdl = SelectKBest(k=labeled_x.shape[1]).fit(labeled_x, preprocessed_y)\n",
    "list(sorted(zip(preprocessed_frame.columns, mdl.scores_), key=lambda f: -f[1]))[:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Алгоритм SelectKBest выбрал 11 наиболее сильно коррелирующих с целевой переменной вещественных признака.\n",
    "С категориальными признаками все не так однозначно. Алгоритм выбрал 10 из 26 наиболее сильно коррелирующих, но эти 10 не находятся в топе (среди 26 лучших) по корреляции с целевой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли межу этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_frame, best_train_labels, best_test_frame, best_test_labels = gb_kbest[3]\n",
    "num_cols_to_drop, cat_cols_to_drop = gb_kbest[5]\n",
    "num_cleaned = numeric_columns.drop(num_cols_to_drop)\n",
    "cat_cleaned = categorial_columns.drop(cat_cols_to_drop)\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_encoder, cat_encoder = frame_to_matrix_kbest(\n",
    "    best_train_frame,\n",
    "    best_test_frame,\n",
    "    best_train_labels,\n",
    "    best_test_labels,\n",
    "    num_cleaned,\n",
    "    cat_cleaned,\n",
    "    21)\n",
    "best_model = gb_kbest[1]\n",
    "# Построим вероятности принадлежности к целевому классу\n",
    "test_probabilities = predict_model_proba(X_test, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>...</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17678</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2Wb19U9</td>\n",
       "      <td>oslk</td>\n",
       "      <td>SQ6wBif</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uWr3</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>am7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ncuRoKG</td>\n",
       "      <td>oslk</td>\n",
       "      <td>aK_wS9i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6XEoI0_</td>\n",
       "      <td>oslk</td>\n",
       "      <td>bSgltRa</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wX53</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1771.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>iokpc3q</td>\n",
       "      <td>d0EEeJi</td>\n",
       "      <td>E2dt_DD</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>nIGXDli</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>882.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>XbZahzR</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>3W_Cq2T</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>R4y5gQQWY8OodqDV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>s03jbzC</td>\n",
       "      <td>oslk</td>\n",
       "      <td>laR6FsY</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uWr3</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>fabLnWA</td>\n",
       "      <td>oslk</td>\n",
       "      <td>EPqQcw6</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22080</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>658.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>zWAdHAC</td>\n",
       "      <td>d0EEeJi</td>\n",
       "      <td>o8HI39x</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TNEC</td>\n",
       "      <td>nIGXDli</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27143</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9835.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4UxGlow</td>\n",
       "      <td>QKW8DRm</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>am7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8d5YWu3</td>\n",
       "      <td>oslk</td>\n",
       "      <td>sXbT3Cb</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4UxGlow</td>\n",
       "      <td>zCkv</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>Qcbd</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ZeaF</td>\n",
       "      <td>mj86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22808</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ALC3PT5</td>\n",
       "      <td>oslk</td>\n",
       "      <td>LsdxH6H</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>I0ktp3J</td>\n",
       "      <td>d0EEeJi</td>\n",
       "      <td>ABtD3EE</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xG3x</td>\n",
       "      <td>PM2D</td>\n",
       "      <td>nIGXDli</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>539.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>xKnYH7D</td>\n",
       "      <td>oslk</td>\n",
       "      <td>jpmOtEt</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>szEZ</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19742</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>749.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4UxGlow</td>\n",
       "      <td>zCkv</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>TCU50_Yjmm6GIBZ0lL_</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17060</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>777.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4UxGlow</td>\n",
       "      <td>oslk</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>mj86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>903.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4UxGlow</td>\n",
       "      <td>QKW8DRm</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25083</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4186.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>FgkM8GC</td>\n",
       "      <td>d0EEeJi</td>\n",
       "      <td>5gS1S5l</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>Aoh3</td>\n",
       "      <td>nIGXDli</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>861.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rSNKYdx</td>\n",
       "      <td>zCkv</td>\n",
       "      <td>O8sb4Lm</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3Cy4</td>\n",
       "      <td>6fzt</td>\n",
       "      <td>SbOd7O8ky1wGNxp0Arj0Xs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22446</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>zxm6hzI</td>\n",
       "      <td>oslk</td>\n",
       "      <td>RRM2fT4</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aoh3</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var9  Var10  Var11   ...    \\\n",
       "17678   NaN   NaN   NaN   NaN   NaN  1001.0   7.0   NaN    NaN    NaN   ...     \n",
       "19991   NaN   NaN   NaN   NaN   NaN   462.0   0.0   NaN    NaN    NaN   ...     \n",
       "2335    NaN   NaN   NaN   NaN   NaN   448.0   0.0   NaN    NaN    NaN   ...     \n",
       "6983    NaN   NaN   NaN   NaN   NaN  1771.0  14.0   NaN    NaN    NaN   ...     \n",
       "7812    NaN   NaN   NaN   NaN   NaN   882.0  14.0   NaN    NaN    NaN   ...     \n",
       "592     NaN   NaN   NaN   NaN   NaN  2884.0   7.0   NaN    NaN    NaN   ...     \n",
       "2051    NaN   NaN   NaN   NaN   NaN  4018.0   7.0   NaN    NaN    NaN   ...     \n",
       "22080   NaN   NaN   NaN   NaN   NaN   658.0   0.0   NaN    NaN    NaN   ...     \n",
       "27143   NaN   NaN   NaN   NaN   NaN  9835.0  28.0   NaN    NaN    NaN   ...     \n",
       "308     NaN   NaN   NaN   NaN   NaN   308.0   0.0   NaN    NaN    NaN   ...     \n",
       "11020   NaN   NaN   NaN   NaN   NaN  1533.0   7.0   NaN    NaN    NaN   ...     \n",
       "22808   NaN   NaN   NaN   NaN   NaN  2660.0   7.0   NaN    NaN    NaN   ...     \n",
       "25583   NaN   NaN   NaN   NaN   NaN  1631.0   7.0   NaN    NaN    NaN   ...     \n",
       "14989   NaN   NaN   NaN   NaN   NaN   539.0   7.0   NaN    NaN    NaN   ...     \n",
       "19742   NaN   NaN   NaN   NaN   NaN   749.0   7.0   NaN    NaN    NaN   ...     \n",
       "17060   NaN   NaN   NaN   NaN   NaN   777.0   7.0   NaN    NaN    NaN   ...     \n",
       "7198    NaN   NaN   NaN   NaN   NaN   903.0   7.0   NaN    NaN    NaN   ...     \n",
       "25083   NaN   NaN   NaN   NaN   NaN  4186.0  21.0   NaN    NaN    NaN   ...     \n",
       "4179    NaN   NaN   NaN   NaN   NaN   861.0   7.0   NaN    NaN    NaN   ...     \n",
       "22446   NaN   NaN   NaN   NaN   NaN  1141.0   7.0   NaN    NaN    NaN   ...     \n",
       "\n",
       "        Var220   Var221   Var222      Var223  Var224  Var225  Var226   Var227  \\\n",
       "17678  2Wb19U9     oslk  SQ6wBif  LM8l689qOp     NaN     NaN    uWr3     RAYp   \n",
       "19991  ncuRoKG     oslk  aK_wS9i         NaN     NaN     NaN    Qu4f     RAYp   \n",
       "2335   6XEoI0_     oslk  bSgltRa  LM8l689qOp     NaN     NaN    wX53     RAYp   \n",
       "6983   iokpc3q  d0EEeJi  E2dt_DD  LM8l689qOp     NaN     NaN    xb3V  nIGXDli   \n",
       "7812   XbZahzR  Al6ZaUT  3W_Cq2T  LM8l689qOp     NaN    ELof    WqMG  02N6s8f   \n",
       "592    s03jbzC     oslk  laR6FsY  LM8l689qOp     NaN     NaN    uWr3     RAYp   \n",
       "2051   fabLnWA     oslk  EPqQcw6  jySVZNlOJy     NaN    kG3k    FSa2     RAYp   \n",
       "22080  zWAdHAC  d0EEeJi  o8HI39x  LM8l689qOp     NaN     NaN    TNEC  nIGXDli   \n",
       "27143  4UxGlow  QKW8DRm  catzS2D  jySVZNlOJy     NaN    kG3k    FSa2     ZI9m   \n",
       "308    8d5YWu3     oslk  sXbT3Cb  LM8l689qOp     NaN    kG3k    WqMG     RAYp   \n",
       "11020  4UxGlow     zCkv  catzS2D  jySVZNlOJy     NaN    ELof    Qcbd     ZI9m   \n",
       "22808  ALC3PT5     oslk  LsdxH6H  LM8l689qOp     NaN     NaN    FSa2     RAYp   \n",
       "25583  I0ktp3J  d0EEeJi  ABtD3EE  LM8l689qOp     NaN    xG3x    PM2D  nIGXDli   \n",
       "14989  xKnYH7D     oslk  jpmOtEt  LM8l689qOp     NaN     NaN    szEZ     RAYp   \n",
       "19742  4UxGlow     zCkv  catzS2D  LM8l689qOp     NaN     NaN    FSa2     ZI9m   \n",
       "17060  4UxGlow     oslk  catzS2D  LM8l689qOp     NaN     NaN    Qu4f     ZI9m   \n",
       "7198   4UxGlow  QKW8DRm  catzS2D  LM8l689qOp     NaN    kG3k    FSa2     ZI9m   \n",
       "25083  FgkM8GC  d0EEeJi  5gS1S5l  LM8l689qOp     NaN    ELof    Aoh3  nIGXDli   \n",
       "4179   rSNKYdx     zCkv  O8sb4Lm  LM8l689qOp     NaN     NaN    3Cy4     6fzt   \n",
       "22446  zxm6hzI     oslk  RRM2fT4  jySVZNlOJy     NaN     NaN    Aoh3     RAYp   \n",
       "\n",
       "                       Var228  Var229  \n",
       "17678                 55YFVY9    am7c  \n",
       "19991           F2FyR07IdsN7I     NaN  \n",
       "2335            F2FyR07IdsN7I     NaN  \n",
       "6983            F2FyR07IdsN7I     NaN  \n",
       "7812         R4y5gQQWY8OodqDV     NaN  \n",
       "592             F2FyR07IdsN7I     NaN  \n",
       "2051            F2FyR07IdsN7I    mj86  \n",
       "22080           F2FyR07IdsN7I     NaN  \n",
       "27143           ib5G6X1eUxUn6    am7c  \n",
       "308             F2FyR07IdsN7I     NaN  \n",
       "11020                    ZeaF    mj86  \n",
       "22808           F2FyR07IdsN7I     NaN  \n",
       "25583           F2FyR07IdsN7I     NaN  \n",
       "14989           F2FyR07IdsN7I     NaN  \n",
       "19742     TCU50_Yjmm6GIBZ0lL_     NaN  \n",
       "17060           ib5G6X1eUxUn6    mj86  \n",
       "7198            ib5G6X1eUxUn6     NaN  \n",
       "25083           F2FyR07IdsN7I    mj86  \n",
       "4179   SbOd7O8ky1wGNxp0Arj0Xs     NaN  \n",
       "22446           F2FyR07IdsN7I     NaN  \n",
       "\n",
       "[20 rows x 211 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list = [0 if l == -1 else 1 for l in best_test_labels.as_matrix().flatten()]\n",
    "indices_by_error = [v[0] for v in sorted(\n",
    "    zip(list(best_test_labels.index), labels_list, test_probabilities),\n",
    "    key=lambda v: -np.abs(v[1]-v[2]))]\n",
    "churn_data_frame.loc[indices_by_error, list(num_cleaned) + list(cat_cleaned)].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что на объектах, дающих наибольшую ошибку очень мало заполненных числовых признаков.\n",
    "Также из графиков, построенных на 1й неделе мы знаем, что разделяющая поверхность между классами довольно размыта, что и объясняет наличие объектов с большой ошибкой.\n",
    "Для улучшения модели можно попробовать отсеивать слабо заполненные объекты (но, как мы помним из кривых обучения не более 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В итоге наилучшее качество я получил, построив модель на основе GradientBoostingClassifier.\n",
    "При предварительном отборе признаков удаляются все константные признаки и признаки не содержащие значений.\n",
    "Для более точного отбора призков используется алгоритм SelectKBest, выбирающий 21 колонку.\n",
    "Вещественные признаки масштабируются с помощью StandardScaler.\n",
    "К категориальным признакам применяется LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_frame = pd.read_csv(\"..\\..\\Data\\churn_data_holdout.csv\", \",\", dtype= { \"Var73\": np.float64 })\n",
    "test_labels_frame = pd.read_csv(\"..\\..\\Data\\churn_labels_holdout.csv\", dtype= { \"labels\": np.int64 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best_21_features = [f[0] for f in sorted(zip(preprocessed_frame.columns, mdl.scores_), key=lambda f: -f[1])][:21]\n",
    "k_best_21_num = pd.Index([\"Var189\", \"Var7\", \"Var73\", \"Var113\", \"Var144\", \"Var13\", \"Var126\", \"Var65\", \"Var74\", \"Var81\", \"Var72\"])\n",
    "k_best_21_cat = pd.Index([\"Var218\", \"Var207\", \"Var229\", \"Var193\", \"Var211\", \"Var205\", \"Var228\", \"Var210\", \"Var227\", \"Var216\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12001, 21), (12001,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_train_frame, best_train_labels, holdout_test_frame, holdout_test_labels, dropped_train_num, dropped_train_cat = process_frame_base(\n",
    "    churn_data_frame[k_best_21_features],\n",
    "    churn_labels_frame,\n",
    "    test_data_frame[k_best_21_features],\n",
    "    test_labels_frame,\n",
    "    k_best_21_num,\n",
    "    k_best_21_cat)\n",
    "\n",
    "train_num_cols = k_best_21_num.drop(dropped_train_num)\n",
    "train_cat_cols = k_best_21_cat.drop(dropped_train_cat)\n",
    "\n",
    "preprocessed_x_train, preprocessed_x_test, preprocessed_y_train, preprocessed_y_test, ne, ce = frame_to_matrix_labeled(\n",
    "    best_train_frame,\n",
    "    holdout_test_frame,\n",
    "    best_train_labels,\n",
    "    holdout_test_labels,\n",
    "    train_num_cols,\n",
    "    train_cat_cols)\n",
    "\n",
    "preprocessed_x_test.shape, preprocessed_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73855722155394998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodel = GradientBoostingClassifier().fit(preprocessed_x_train, preprocessed_y_train)\n",
    "final_probabilities = predict_model_proba(preprocessed_x_test, bestmodel)\n",
    "finalRocAuc = roc_auc_score(preprocessed_y_test, final_probabilities)\n",
    "finalRocAuc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Попробовать еще поподбирать модели (например SVM, LogisticRegression).\n",
    "- Более скрупулезно подойти к подбору параметров модели и подбирать их по-одному, а не все скопом.\n",
    "- Отбирать категориальные и вещественные признаки разными алгоритмами.\n",
    "- Поискать сильно скоррелированные друг с другом признаки и удалить их.\n",
    "- Попробовать совсем другие способы отбора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
