{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение и оптимизация модели\n",
    "\n",
    "В этом задании вам предстоит поучаствовать в соревновании на [kaggle inclass](https://inclass.kaggle.com/c/telecom-clients-churn-prediction).\n",
    "В соревновании вы будете работать с той же выборкой, что и ранее, поэтому воспользуйтесь результатами, полученными на предыдущих неделях. Для успешного участия в соревновании необходимо преодолеть по качеству beseline решение.\n",
    "\n",
    "Итак, мы научились обрабатывать данные, выбрали схему кросс-валидации и определились с метриками качества. Пора переходить к оптимизации модели. На этой неделе вам предстоит принять участие в соревновании на платформе kaggle inclass! Цель такого соревнования - преодолеть предложенное baseline решение, а, главное, обсудить и сравнить предложенные решения на форуме. Какие признаки оказали наибольший вклад в модель? Как лучше обрабатывать категориальные признаки? Нужно ли делать отбор признаков, А балансировать выборку? Экспериментируйте с данными и обсуждайте ваши решения на форуме!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, recall_score, precision_score, log_loss\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "seed = 1903\n",
    "first_categorial_index = 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27999, 230)\n",
      "(27999, 1)\n"
     ]
    }
   ],
   "source": [
    "churn_data_frame = pd.read_csv(\"..\\..\\Data\\churn_data_train.csv\", \",\")\n",
    "churn_labels_frame = pd.read_csv(\"..\\..\\Data\\churn_labels_train.csv\")\n",
    "kaggle_data_frame = pd.read_csv(\"..\\..\\Data\\orange_small_kaggle_test_data.csv\", \",\")\n",
    "print(churn_data_frame.shape)\n",
    "print(churn_labels_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая предобработка признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numericna(train_frame, test_frame, averageCalculator):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме значениями, посчитанными averageCalculator. \"\"\"\n",
    "    \n",
    "    # Посчитаем средние по колонкам\n",
    "    numeric_avgs = averageCalculator(train_frame)\n",
    "    \n",
    "    # Оставим только те колонки, в которых среднее значение не равно NaN, т.к. в таких колонках совсем нет значений\n",
    "    numeric_avgs = numeric_avgs.dropna()\n",
    "    dropped_columns = train_frame.columns.drop(numeric_avgs.index)\n",
    "    n_frame_train = train_frame[list(numeric_avgs.index)]\n",
    "    n_frame_test = test_frame[list(numeric_avgs.index)]\n",
    "    \n",
    "    # Заполним пропущенные численные значения средними\n",
    "    n_frame_train = n_frame_train.fillna(numeric_avgs, axis=0)\n",
    "    n_frame_test = n_frame_test.fillna(numeric_avgs, axis=0)\n",
    "    return (n_frame_train, n_frame_test, dropped_columns)\n",
    "\n",
    "def fill_numericna_means(train_frame, test_frame):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме средними и удаляет те колонки, в которых значений нет. \"\"\"\n",
    "    return fill_numericna(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        lambda f: f.mean(axis=0, skipna=True))\n",
    "\n",
    "def fill_numericna_medians(train_frame, test_frame):\n",
    "    \"\"\" Функция заполняет значения в числовом фрейме медианами и удаляет те колонки, в которых значений нет. \"\"\"\n",
    "    return fill_numericna(\n",
    "        train_frame,\n",
    "        test_frame,\n",
    "        lambda f: f.median(axis=0, skipna=True))\n",
    "    \n",
    "def remove_constant_features(frame, min_count=2):\n",
    "    \"\"\"Функция удаляет колонки, которые содержат только одно значение.\"\"\"\n",
    "    \n",
    "    # Посчитаем количества уникальных значений по колонкам\n",
    "    unique_counts = frame.nunique()\n",
    "    # Удалим колонки с количеством значений меньшим min_count\n",
    "    columns_to_drop = unique_counts[unique_counts < min_count].index\n",
    "    \n",
    "    return (frame.drop(columns=columns_to_drop), columns_to_drop)\n",
    "\n",
    "def fill_na_frequent_values(frame):\n",
    "    \"\"\" Функция заполняет пустые значения самым частым значением в колонке. \"\"\"\n",
    "    result = frame.copy()\n",
    "    for column in result.columns:\n",
    "        frequencies = result[column].value_counts()\n",
    "        if (len(frequencies) < 1):\n",
    "            continue\n",
    "        most_frequent_value = frequencies.index[0]\n",
    "        result[column] = result[column].fillna(most_frequent_value)\n",
    "    return result\n",
    "\n",
    "class MatrixLabelEncoder:\n",
    "    \"\"\" Класс кодирует категории числами от 0 до n, где n количество категорий в колонке. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoders = []\n",
    "    \n",
    "    def fit(self, matrix):\n",
    "        for column_number in range(matrix.shape[1]):\n",
    "            column = matrix[:,column_number]\n",
    "            labelEncoder = LabelEncoder().fit(column)\n",
    "            self.encoders.append(labelEncoder)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, matrix):\n",
    "        transformed = np.empty(matrix.shape)\n",
    "        for column_number in range(matrix.shape[1]):\n",
    "            labelEncoder = self.encoders[column_number]\n",
    "            num_column = labelEncoder.transform(matrix[:,column_number])\n",
    "            for row_number, val in enumerate(num_column):\n",
    "                transformed[row_number, column_number] = val\n",
    "        return transformed\n",
    "    \n",
    "class CompositeEncoder:\n",
    "    def __init__(self, encoder_factories):\n",
    "        self.encoder_factories = encoder_factories\n",
    "        \n",
    "    def fit(self, matrix):\n",
    "        encoders = []\n",
    "        transformed = matrix\n",
    "        for encoder_factory in self.encoder_factories:\n",
    "            encoder = encoder_factory().fit(transformed)\n",
    "            encoders.append(encoder)\n",
    "            transformed = encoder.transform(transformed)\n",
    "        self.encoders = encoders\n",
    "        return self\n",
    "\n",
    "    def transform(self, matrix):\n",
    "        for encoder in self.encoders:\n",
    "            matrix = encoder.transform(matrix)\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим числовые и категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = churn_data_frame.columns[:first_categorial_index]\n",
    "categorial_columns = churn_data_frame.columns[first_categorial_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ridge_proba(X, model):\n",
    "    \"\"\" Функция возвращает вероятности предсказаний для класса churn модель Ridge \"\"\"\n",
    "    # Поскольку RidgeClassifier не обладает функцией predict_proba приходится считать его вручную\n",
    "    # Подробнее можно посмотреть здесь:\n",
    "    # https://www.codesd.com/item/scikit-learn-ridge-classifier-extract-class-probabilities.html\n",
    "    func = model.decision_function(X)\n",
    "    return np.exp(func) / (1 + np.exp(func))\n",
    "\n",
    "def predict_model_proba(X, model):\n",
    "    \"\"\" Функция возвращает вероятности предсказаний для класса churn \"\"\"\n",
    "    return list(zip(*model.predict_proba(X)))[1]\n",
    "\n",
    "def get_model_data(model_with_data):\n",
    "    \"\"\" Функция принимает на вход результат stratifiedKFold_fscore и возвращает только модель и её X_test и y_test \"\"\"\n",
    "    model = model_with_data[1]\n",
    "    split = model_with_data[3]\n",
    "    X = split[2]\n",
    "    y = split[3]\n",
    "    return (model,X,y)\n",
    "\n",
    "def calculate_metrics(model_with_data, predict_probabilities):\n",
    "    \"\"\" Посчитаем метрики качества для модели \"\"\"\n",
    "    model, X, y = get_model_data(model_with_data)\n",
    "    predictions = model.predict(X)\n",
    "    probabilities = predict_probabilities(X, model)\n",
    "    # Считаем F-меру, precision и recall\n",
    "    fscore = f1_score(y, predictions)\n",
    "    precision = precision_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "    # Считаем Log loss\n",
    "    logLoss = log_loss(y, probabilities)\n",
    "    # Считаем roc auc score\n",
    "    rocAuc = roc_auc_score(y, probabilities)\n",
    "    return (precision,recall,fscore,logLoss,rocAuc)\n",
    "\n",
    "def transform_to_chart_model(label, model_with_data, predict_probabilities):\n",
    "    \"\"\" Функция принимает на вход заголовок, результат функции stratifiedKFold_fscore и функцию, считающую вероятности\n",
    "    и возвращает заголовок, вектор ответов и вектор вероятностей для построения графиков \"\"\"\n",
    "    model, X, y = get_model_data(model_with_data)\n",
    "    probabilities = predict_probabilities(X, model)\n",
    "    return (label, y, probabilities)\n",
    "\n",
    "def charts_row(model_charts, chart_builder, figsize=(11, 4)):\n",
    "    \"\"\" Функция принимает данные для построения графиков и логику построения одного графика и строит графики в одну строку \"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=len(model_charts),\n",
    "        figsize=figsize,\n",
    "        sharey=True)\n",
    "    for i, chart_data in enumerate(model_charts):\n",
    "        ax = axes[i]\n",
    "        chart_builder(chart_data, ax, i)\n",
    "    plt.show()\n",
    "    \n",
    "def scatter(chart_data, ax, ax_index, T=0.5):\n",
    "    \"\"\" Функция строит график распределения вероятностей по классам \"\"\"\n",
    "    label, actual, predicted = chart_data\n",
    "    ax.scatter(actual, predicted)\n",
    "    ax.set_xlabel(\"Labels\")\n",
    "    if ax_index == 0:\n",
    "        ax.set_ylabel(\"Predicted probabilities\")\n",
    "    ax.set_title(label)\n",
    "    ax.plot([-1.1, 1.2], [T, T])\n",
    "    ax.axis([-1.1, 1.1, -0.1, 1.1])\n",
    "    \n",
    "def precision_recal_thresh(chart_data, ax, ax_index):\n",
    "    \"\"\" Функция строит графики значений precision и recall в зависимости от порога \"\"\"\n",
    "    label, actual, predicted = chart_data\n",
    "    prec, rec, thresh = precision_recall_curve(actual, predicted)\n",
    "    min_len = min([len(prec),len(rec),len(thresh)])\n",
    "    ax.plot(thresh[:min_len], prec[:min_len], label=\"precision\")\n",
    "    ax.plot(thresh[:min_len], rec[:min_len], label=\"recall\")\n",
    "    ax.legend(loc=1)\n",
    "    ax.set_xlabel(\"threshold\")\n",
    "    ax.set_title(label)\n",
    "    \n",
    "def auc_prc(chart_data, ax, ax_index):\n",
    "    \"\"\" Функция строит график AUC PRC (зависимость precision от recall)\"\"\"\n",
    "    label, actual, predicted = chart_data\n",
    "    prec, rec, thresh = precision_recall_curve(actual, predicted)\n",
    "    min_len = min([len(prec),len(rec),len(thresh)])\n",
    "    ax.plot(rec[:min_len], prec[:min_len])\n",
    "    ax.set_xlabel(\"recall\")\n",
    "    if ax_index == 0:\n",
    "        ax.set_ylabel(\"precision\")\n",
    "    ax.set_title(label)\n",
    "\n",
    "def calc_min_distance (actual,predicted):\n",
    "    \"\"\" Функция считает минимальное расстояние до точки [0;1] для AUC ROC \"\"\"\n",
    "    fpr,tpr,thr = roc_curve(actual,predicted)\n",
    "    distance,fpr_v,tpr_v,thr_v = min(zip(np.sqrt((1.-tpr)**2+fpr**2),fpr,tpr,thr), key=lambda d:d[0])\n",
    "    return (distance,fpr_v,tpr_v,thr_v)\n",
    "    \n",
    "def auc_roc(chart_data, ax, ax_index):\n",
    "    \"\"\" Функция строит кривую AUC ROC и отмечает точку, соответствующую минимальному расстоянию до точки [0;1] \"\"\"\n",
    "    label, actual, predicted = chart_data\n",
    "    fpr, tpr, thr = roc_curve(actual, predicted)\n",
    "    min_dist,min_fpr,min_tpr,_ = calc_min_distance(actual, predicted)\n",
    "    ax.plot(fpr, tpr, label=\"ROC AUC curve\")\n",
    "    ax.scatter(min_fpr,min_tpr,color=\"red\")\n",
    "    ax.set_xlabel(\"false positive rate\")\n",
    "    if ax_index == 0:\n",
    "        ax.set_ylabel(\"true positive rate\")\n",
    "    ax.legend(loc=4)\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratifiedKFold_fscore(\n",
    "    frame,\n",
    "    labels,\n",
    "    model_factory,\n",
    "    process_frame,\n",
    "    frame_to_matrix,\n",
    "    numeric_features,\n",
    "    categorial_features,\n",
    "    predict_probabilities,\n",
    "    seed,\n",
    "    folds_count = 3):\n",
    "    \"\"\" Функция разбивает набор данных на folds_count, считает ROC-AUC на каждом фолде\n",
    "        и возвращает усредненное по фолдам значение.\n",
    "        Функция также возвращает модель, показавшую лучшее качество, её метрики и разделение данных.\n",
    "        Разделение данных нужно для того, чтобы строить метрики модели на данных, на которых она не обучалась.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=folds_count, shuffle=True, random_state=seed)\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_table = None\n",
    "    best_split = None\n",
    "    best_encoders = None\n",
    "    best_dropped_columns = None\n",
    "    metrics_sum = 0\n",
    "    for train_indices, test_indices in skf.split(frame, labels):\n",
    "        # Разобьем фрем на train и test с помощью функции process_frame\n",
    "        # Внутри такой функции мы можем по-разному обрабатывать признаки обучаясь только на train наборе.\n",
    "        train_frame, train_labels, test_frame, test_labels, dropped_numeric, dropped_categorial = process_frame(\n",
    "            frame.loc[train_indices, :],\n",
    "            labels.loc[train_indices, :],\n",
    "            frame.loc[test_indices, :],\n",
    "            labels.loc[test_indices, :],\n",
    "            numeric_features,\n",
    "            categorial_features)\n",
    "        numeric_cleaned = numeric_features.drop(dropped_numeric)\n",
    "        categorial_cleaned = categorial_features.drop(dropped_categorial)\n",
    "        # Преобразуем фреймы в матрицы.\n",
    "        # Тут можно выполнить финальное преобразование признаков, например масштабирование признаков.\n",
    "        # В функции frame_to_matrix энкодеры типа StandardScaler обучаются только на train признаках.\n",
    "        X_train, X_test, num_encoder, cat_encoder = frame_to_matrix(\n",
    "            train_frame,\n",
    "            test_frame,\n",
    "            numeric_cleaned,\n",
    "            categorial_cleaned)\n",
    "        y_train = train_labels.as_matrix().flatten()\n",
    "        y_test = test_labels.as_matrix().flatten()\n",
    "\n",
    "        model = model_factory()\n",
    "        # Обучим модель\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Построим вероятности принадлежности к целевому классу\n",
    "        probabilities = predict_probabilities(X_test, model)\n",
    "        # Считаем roc auc score\n",
    "        rocAuc = roc_auc_score(y_test, probabilities)\n",
    "        metrics_sum += rocAuc\n",
    "        if(best_model is None or best_score < rocAuc):\n",
    "            # В случае, если модель лучше предыдущих сохраним её\n",
    "            # оценку, модель, матрицу ошибок и разделение данных\n",
    "            best_score = rocAuc\n",
    "            best_model = model\n",
    "            best_dropped_columns = (dropped_numeric, dropped_categorial)\n",
    "            best_encoders = (num_encoder, cat_encoder)\n",
    "            best_split = (X_train, y_train, X_test, y_test)\n",
    "    return (\n",
    "        metrics_sum/folds_count,\n",
    "        best_model,\n",
    "        best_score,\n",
    "        best_split,\n",
    "        best_encoders,\n",
    "        best_dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_frame_common(frame, numeric_features, categorial_features):\n",
    "    \"\"\"Функция делит признакина числовые и категориальные и удаляет константные признаки, содержащие только одно значение\"\"\"\n",
    "    # Разделим коллекции на группы - числовые и категориальные.\n",
    "    numeric_frame = frame[numeric_features].copy()\n",
    "    categorial_frame = frame[categorial_features].copy()\n",
    "    # Удалим вещественные колонки, содержащие одно и менее значений. 0 значений мы получаем, когда значения во всех строках Nan.\n",
    "    numeric_frame_no_const, dropped_const_numeric_columns = remove_constant_features(numeric_frame)\n",
    "    \n",
    "    # Удалим категориальные колонки, содержащие ноль значений. Если есть одно значение, то могут быть Nan, которые для\n",
    "    # категориальных признаков могут быть еще одной категорией (зависит от стратегии обработки).\n",
    "    categorial_frame_no_const, dropped_const_categorial_columns = remove_constant_features(categorial_frame, 1)\n",
    "    \n",
    "    # Восстановим фрейм и вернем вместе с ним список удаленных категориальных колонок.\n",
    "    return (pd.concat([numeric_frame, categorial_frame], axis=1),\n",
    "            list(dropped_const_numeric_columns),\n",
    "            list(dropped_const_categorial_columns))\n",
    "\n",
    "def process_frame_base_model(train_frame, train_labels, test_frame, test_labels, numeric_features, categorial_features):\n",
    "    \"\"\" Функция строит базовую модель из предыдущей недели \"\"\"\n",
    "    \n",
    "    # Удалим константные колонки из train_frame, и такие-же колонки из test_frame\n",
    "    train_frame, const_numeric_columns, const_categorial_columns = cleanup_frame_common(\n",
    "        train_frame,\n",
    "        numeric_features,\n",
    "        categorial_features)\n",
    "    test_frame = test_frame.drop(columns=const_numeric_columns)\n",
    "    test_frame = test_frame.drop(columns=const_categorial_columns)\n",
    "    \n",
    "    numeric_features = numeric_features.drop(const_numeric_columns)\n",
    "    categorial_features = categorial_features.drop(const_categorial_columns)\n",
    "    \n",
    "    # Заполним пропущенные вещественные значения средними\n",
    "    numeric_train, numeric_test, dropped_numeric = fill_numericna_means(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    numeric_features = numeric_features.drop(dropped_numeric)\n",
    "    \n",
    "    # Заполним пропущенные категориальные значения строками \"NaV\" (Not a value)\n",
    "    categorial_train = train_frame[categorial_features].fillna(\"NaV\")\n",
    "    categorial_test = test_frame[categorial_features].fillna(\"NaV\")\n",
    "    \n",
    "    # Удалим категориальные колонки с одним единственным значением\n",
    "    categorial_train, dropped_categorial = remove_constant_features(categorial_train)\n",
    "    categorial_test = categorial_test.drop(columns=dropped_categorial)\n",
    "    \n",
    "    categorial_features = categorial_features.drop(dropped_categorial)\n",
    "    \n",
    "    # Список удаленных колонок\n",
    "    dropped_numeric = np.concatenate([\n",
    "        list(const_numeric_columns),\n",
    "        list(dropped_numeric)])\n",
    "    dropped_categorial = np.concatenate([\n",
    "        list(const_categorial_columns),\n",
    "        list(dropped_categorial)])\n",
    "    \n",
    "    return (pd.concat([numeric_train, categorial_train], axis=1),\n",
    "            train_labels,\n",
    "            pd.concat([numeric_test, categorial_test], axis=1),\n",
    "            test_labels,\n",
    "            dropped_numeric,\n",
    "            dropped_categorial)\n",
    "\n",
    "def scale_features(train_frame, test_frame):\n",
    "    train_numeric = train_frame.as_matrix()\n",
    "    \n",
    "    scaler = StandardScaler().fit(train_numeric)\n",
    "    \n",
    "    train_numeric = coo_matrix(scaler.transform(train_numeric))\n",
    "    test_numeric = coo_matrix(scaler.transform(test_frame.as_matrix()))\n",
    "    \n",
    "    return (train_numeric, test_numeric, scaler)\n",
    "\n",
    "def one_hot_features(train_frame, test_frame):\n",
    "    fit_matrix = pd.concat([train_frame, test_frame]).as_matrix()\n",
    "    \n",
    "    if fit_matrix.shape[0] == 0 or fit_matrix.shape[1] == 0:\n",
    "        return (coo_matrix(train_frame.as_matrix()), coo_matrix(test_frame.as_matrix()), None)\n",
    "    categorial_encoder = CompositeEncoder([MatrixLabelEncoder, OneHotEncoder]).fit(fit_matrix)\n",
    "    \n",
    "    train_categorial = categorial_encoder.transform(train_frame.as_matrix())\n",
    "    test_categorial = categorial_encoder.transform(test_frame.as_matrix())\n",
    "    \n",
    "    return (train_categorial, test_categorial, categorial_encoder)\n",
    "\n",
    "def int_label_features(train_frame, test_frame):\n",
    "    fit_matrix = pd.concat([train_frame, test_frame]).as_matrix()\n",
    "    categorial_encoder = MatrixLabelEncoder().fit(fit_matrix)\n",
    "    \n",
    "    train_categorial = categorial_encoder.transform(train_frame.as_matrix())\n",
    "    test_categorial = categorial_encoder.transform(test_frame.as_matrix())\n",
    "    \n",
    "    return (train_categorial, test_categorial, categorial_encoder)\n",
    "\n",
    "def frame_to_matrix_one_hot(train_frame, test_frame, numeric_features, categorial_features):\n",
    "    \"\"\" Функци преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные с помощью OneHotEncoding. \"\"\"\n",
    "    \n",
    "    # Масштабируем вещественные признаки\n",
    "    train_numeric, test_numeric, scaler = scale_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    # One hot encode для категориальных признаков\n",
    "    train_categorial, test_categorial, categorial_encoder = one_hot_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            scaler,\n",
    "            categorial_encoder)\n",
    "\n",
    "def frame_to_matrix_labeled(train_frame, test_frame, numeric_features, categorial_features):\n",
    "    \"\"\" Функция преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные целыми числами. \"\"\"\n",
    "    \n",
    "    # Масштабируем вещественные признаки\n",
    "    train_numeric, test_numeric, scaler = scale_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    train_categorial, test_categorial, categorial_encoder = int_label_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            scaler,\n",
    "            categorial_encoder)\n",
    "\n",
    "def ridge_one_hot_builder(frame, labels):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        process_frame_base_model,\n",
    "        frame_to_matrix_one_hot, \n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        predict_ridge_proba,\n",
    "        seed)\n",
    "\n",
    "def random_forest_labeled_builder(frame, labels):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RandomForestClassifier,\n",
    "        process_frame_base_model,\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def gradient_boosting_labeled_builder(frame, labels):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        process_frame_base_model,\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовые модели, построенные на предыдущей неделе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_one_hot_base = ridge_one_hot_builder(churn_data_frame, churn_labels_frame)\n",
    "random_forest_base = random_forest_labeled_builder(churn_data_frame, churn_labels_frame)\n",
    "gradient_boosting_base = gradient_boosting_labeled_builder(churn_data_frame, churn_labels_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "Проверим увеличит ли сэмплирование данных качество модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_frame_sampling(frame_to_balance, labels, repeat_times):\n",
    "    \"\"\" Функция балансирует соотношения классов сэмплируя класс churn. \"\"\"\n",
    "    y_name = \"labels\"\n",
    "    churn_only_labels = labels[labels[y_name] == 1]\n",
    "    churn_indices = list(churn_only_labels.index)\n",
    "    churn_only_frame = frame_to_balance.loc[churn_indices,:]\n",
    "    churn_balanced_frame = frame_to_balance.copy()\n",
    "    churn_balanced_labels = labels.copy()\n",
    "    for i in range(0, (repeat_times-1)):\n",
    "        churn_balanced_frame = pd.concat([churn_balanced_frame, churn_only_frame], ignore_index=True)\n",
    "        churn_balanced_labels = pd.concat([churn_balanced_labels, churn_only_labels], ignore_index=True)\n",
    "        \n",
    "    churn_balanced_frame[y_name] = churn_balanced_labels[y_name]\n",
    "    churn_balanced_frame = churn_balanced_frame.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    churn_balanced_labels = pd.DataFrame(churn_balanced_frame[y_name], columns=[y_name])\n",
    "    churn_balanced_frame = churn_balanced_frame.drop(columns=[y_name])\n",
    "    \n",
    "    return (churn_balanced_frame, churn_balanced_labels)\n",
    "\n",
    "def balance_train_frame(train_frame, train_labels, test_frame, test_labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция балансирует train frame, после этого обрабатывает признаки с помощью метода\n",
    "        process_frame_base_model \"\"\"\n",
    "    balanced_train, balanced_labels = balance_frame_sampling(train_frame, train_labels, repeat_times)\n",
    "    return process_frame_base_model(balanced_train, balanced_labels, test_frame, test_labels, numeric_features, categorial_features)\n",
    "\n",
    "def balanced_ridge_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция строит модель на основе RidgeClassifier, предварительно балансируя выборки.\"\"\"\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_one_hot,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_ridge_proba,\n",
    "        seed)\n",
    "\n",
    "def balanced_random_forest_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция строит модель на основе RandomForestClassifier, предварительно балансируя выборки.\"\"\"\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RandomForestClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def balanced_gradient_boosting_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция строит модель на основе GradientBoostingClassifier, предварительно балансируя выборки.\"\"\"\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_labeled,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим RidgeClassifier, RandomForestClassifier и GradientBoostingClassifier и сравним результат с базовым решением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_times = 12\n",
    "ridge_balanced = balanced_ridge_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    repeat_times)\n",
    "random_forest_balanced = balanced_random_forest_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    repeat_times)\n",
    "gradient_boosting_balanced = balanced_gradient_boosting_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    numeric_columns,\n",
    "    categorial_columns,\n",
    "    repeat_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Mean      Best\n",
      "Ridge Base                  0.675126  0.686025\n",
      "Ridge Balanced              0.671609  0.683472\n",
      "Random Forest Base          0.588253  0.600968\n",
      "Random Forest Balanced      0.610274  0.614934\n",
      "Gradient boosting base      0.732486  0.742115\n",
      "Gradient boosting balanced  0.723209  0.734313\n",
      "\n",
      "                            Precision    Recall   F-Score  Log Loss   Roc Auc\n",
      "Ridge Base                   0.000000  0.000000  0.000000  0.414959  0.686025\n",
      "Ridge Balanced               0.130529  0.522302  0.208861  0.611811  0.683472\n",
      "Random Forest Base           0.333333  0.004323  0.008535  1.017398  0.600968\n",
      "Random Forest Balanced       0.280000  0.010072  0.019444  0.809283  0.614934\n",
      "Gradient boosting base       0.187500  0.004323  0.008451  0.240476  0.742115\n",
      "Gradient boosting balanced   0.143142  0.621037  0.232659  0.556841  0.734313\n"
     ]
    }
   ],
   "source": [
    "indices = [\"Ridge Base\", \"Ridge Balanced\", \"Random Forest Base\", \"Random Forest Balanced\", \"Gradient boosting base\", \"Gradient boosting balanced\"]\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        [ridge_one_hot_base[0], ridge_one_hot_base[2]],\n",
    "        [ridge_balanced[0], ridge_balanced[2]],\n",
    "        [random_forest_base[0], random_forest_base[2]],\n",
    "        [random_forest_balanced[0], random_forest_balanced[2]],\n",
    "        [gradient_boosting_base[0], gradient_boosting_base[2]],\n",
    "        [gradient_boosting_balanced[0], gradient_boosting_balanced[2]]\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Mean\", \"Best\"]))\n",
    "print()\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        calculate_metrics(ridge_one_hot_base, predict_ridge_proba),\n",
    "        calculate_metrics(ridge_balanced, predict_ridge_proba),\n",
    "        calculate_metrics(random_forest_base, predict_model_proba),\n",
    "        calculate_metrics(random_forest_balanced, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_base, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_balanced, predict_model_proba),\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Precision\", \"Recall\", \"F-Score\", \"Log Loss\", \"Roc Auc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из таблицы выше видно, что при балансировке классов стабильно растет Recall и F-Score. Roc Auc при этом просел аж для двух моделей (Ridge и Gradient Boosting).\n",
    "Log Loss для Gradient Boosting при балансироваке также вырос.\n",
    "Судя по тому, что по большей части метрики растут и растут сильно (в десятках процентов) я в данном ноутбуке буду балансировать классы.\n",
    "\n",
    "## Масштабировать или нет\n",
    "\n",
    "Попробуем не масшатабировать модель. Сравним результаты и решим, что лучше, - масштабировать признаки или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix_one_hot_not_scaled(train_frame, test_frame, numeric_features, categorial_features):\n",
    "    \"\"\" Функци преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные с помощью OneHotEncoding. \"\"\"\n",
    "    \n",
    "    train_numeric = coo_matrix(train_frame[numeric_features].as_matrix())\n",
    "    test_numeric = coo_matrix(test_frame[numeric_features].as_matrix())\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    # One hot encode для категориальных признаков\n",
    "    train_categorial, test_categorial, categorial_encoder = one_hot_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            None,\n",
    "            categorial_encoder)\n",
    "\n",
    "def to_matrix_labeled_not_scaled(train_frame, test_frame, numeric_features, categorial_features):\n",
    "    \"\"\" Функция преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки и кодирует категориальные целыми числами. \"\"\"\n",
    "    \n",
    "    train_numeric = coo_matrix(train_frame[numeric_features].as_matrix())\n",
    "    test_numeric = coo_matrix(test_frame[numeric_features].as_matrix())\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    train_categorial, test_categorial, categorial_encoder = int_label_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            None,\n",
    "            categorial_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_scaled_bal_ridge_builder(frame, labels):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, 12),\n",
    "        to_matrix_one_hot_not_scaled,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        predict_ridge_proba,\n",
    "        seed)\n",
    "\n",
    "def not_scaled_bal_random_forest_builder(frame, labels):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RandomForestClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, 12),\n",
    "        to_matrix_labeled_not_scaled,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def not_scaled_bal_gradient_boosting_builder(frame, labels):\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, 12),\n",
    "        to_matrix_labeled_not_scaled,\n",
    "        numeric_columns,\n",
    "        categorial_columns,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_not_scaled = not_scaled_bal_ridge_builder(churn_data_frame, churn_labels_frame)\n",
    "random_forest_not_scaled = not_scaled_bal_random_forest_builder(churn_data_frame, churn_labels_frame)\n",
    "gradient_boosting_not_scaled = not_scaled_bal_gradient_boosting_builder(churn_data_frame, churn_labels_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Mean      Best\n",
      "Ridge Balanced                0.204751  0.208801\n",
      "Ridge Not Scaled              0.152178  0.156084\n",
      "Random Forest Balanced        0.021115  0.024862\n",
      "Random Forest Not Scaled      0.015695  0.019499\n",
      "Gradient boosting balanced    0.232492  0.234680\n",
      "Gradient boosting Not Scaled  0.232407  0.234615\n",
      "\n",
      "                              Precision    Recall   F-Score  Log Loss  \\\n",
      "Ridge Balanced                 0.130482  0.522302  0.208801  0.611813   \n",
      "Ridge Not Scaled               0.090996  0.548201  0.156084  0.679135   \n",
      "Random Forest Balanced         0.300000  0.012968  0.024862  0.842268   \n",
      "Random Forest Not Scaled       0.304348  0.010072  0.019499  0.905647   \n",
      "Gradient boosting balanced     0.145041  0.614388  0.234680  0.548544   \n",
      "Gradient boosting Not Scaled   0.144992  0.614388  0.234615  0.548541   \n",
      "\n",
      "                               Roc Auc  \n",
      "Ridge Balanced                0.683467  \n",
      "Ridge Not Scaled              0.574823  \n",
      "Random Forest Balanced        0.602864  \n",
      "Random Forest Not Scaled      0.600328  \n",
      "Gradient boosting balanced    0.724857  \n",
      "Gradient boosting Not Scaled  0.724861  \n"
     ]
    }
   ],
   "source": [
    "indices = [\"Ridge Balanced\", \"Ridge Not Scaled\", \"Random Forest Balanced\",\n",
    "           \"Random Forest Not Scaled\", \"Gradient boosting balanced\", \"Gradient boosting Not Scaled\"]\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        [ridge_balanced[0], ridge_balanced[2]],\n",
    "        [ridge_not_scaled[0], ridge_not_scaled[2]],\n",
    "        [random_forest_balanced[0], random_forest_balanced[2]],\n",
    "        [random_forest_not_scaled[0], random_forest_not_scaled[2]],\n",
    "        [gradient_boosting_balanced[0], gradient_boosting_balanced[2]],\n",
    "        [gradient_boosting_not_scaled[0], gradient_boosting_not_scaled[2]]\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Mean\", \"Best\"]))\n",
    "print()\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        calculate_metrics(ridge_balanced, predict_ridge_proba),\n",
    "        calculate_metrics(ridge_not_scaled, predict_ridge_proba),\n",
    "        calculate_metrics(random_forest_balanced, predict_model_proba),\n",
    "        calculate_metrics(random_forest_not_scaled, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_balanced, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_not_scaled, predict_model_proba),\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Precision\", \"Recall\", \"F-Score\", \"Log Loss\", \"Roc Auc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основном метрики либо не растут, либо уменьшаются. На мой взгляд лучше масштабировать признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полиномиальные фичи\n",
    "Перед отбором признаков добавим к числовым признакам их квадраты и попарные произведения, тем самым перейдем в спрямляющее пространство.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_features(train_frame, test_frame):\n",
    "    train_numeric = train_frame.as_matrix()\n",
    "    \n",
    "    numeric_encoder = CompositeEncoder([StandardScaler, PolynomialFeatures]).fit(train_numeric)\n",
    "    \n",
    "    train_numeric = coo_matrix(numeric_encoder.transform(train_numeric))\n",
    "    test_numeric = coo_matrix(numeric_encoder.transform(test_frame.as_matrix()))\n",
    "    \n",
    "    return (train_numeric, test_numeric, numeric_encoder)\n",
    "\n",
    "def frame_to_matrix_one_hot_poly(train_frame, test_frame, numeric_features, categorial_features):\n",
    "    \"\"\" Функция преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки, добавляет их квадраты и попарные произведения.\n",
    "        Кодирует категориальные признаки с помощью OneHotEncoding. \"\"\"\n",
    "    \n",
    "    # Масштабируем вещественные признаки и добавим к ним их квадраты и попарные произведения\n",
    "    train_numeric, test_numeric, numeric_encoder = add_polynomial_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    # One hot encode для категориальных признаков\n",
    "    train_categorial, test_categorial, categorial_encoder = one_hot_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            numeric_encoder,\n",
    "            categorial_encoder)\n",
    "\n",
    "def frame_to_matrix_labeled_poly(train_frame, test_frame, numeric_features, categorial_features):\n",
    "    \"\"\" Функция преобразует фрейм к sparse матрице.\n",
    "        Масштабирует вещественные признаки, добавляет их квадраты и попарные произведения.\n",
    "        Кодирует категориальные признаки целыми числами. \"\"\"\n",
    "    \n",
    "    # Масштабируем вещественные признаки и добавим к ним их квадраты и попарные произведения\n",
    "    train_numeric, test_numeric, numeric_encoder = add_polynomial_features(\n",
    "        train_frame[numeric_features],\n",
    "        test_frame[numeric_features])\n",
    "    \n",
    "    # Закодируем категориальные признаки значениями от 0 до n с помощью MatrixLabelEncoder\n",
    "    train_categorial, test_categorial, categorial_encoder = int_label_features(\n",
    "        train_frame[categorial_features],\n",
    "        test_frame[categorial_features])\n",
    "    \n",
    "    return (hstack([train_numeric, train_categorial]),\n",
    "            hstack([test_numeric, test_categorial]),\n",
    "            numeric_encoder,\n",
    "            categorial_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_ridge_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция строит модель на основе RidgeClassifier, добавляя полиномиальные признаки.\"\"\"\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RidgeClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_one_hot_poly,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_ridge_proba,\n",
    "        seed)\n",
    "\n",
    "def poly_random_forest_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция строит модель на основе RandomForestClassifier, добавляя полиномиальные признаки.\"\"\"\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        RandomForestClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_labeled_poly,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)\n",
    "\n",
    "def poly_gradient_boosting_builder(frame, labels, numeric_features, categorial_features, repeat_times):\n",
    "    \"\"\" Функция строит модель на основе GradientBoostingClassifier, добавляя полиномиальные признаки.\"\"\"\n",
    "    return stratifiedKFold_fscore(\n",
    "        frame,\n",
    "        labels,\n",
    "        GradientBoostingClassifier,\n",
    "        lambda tf, tl, tsf, tsl, nf, cf: balance_train_frame(tf, tl, tsf, tsl, nf, cf, repeat_times),\n",
    "        frame_to_matrix_labeled_poly,\n",
    "        numeric_features,\n",
    "        categorial_features,\n",
    "        predict_model_proba,\n",
    "        seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_poly = poly_ridge_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    very_best_num_features,\n",
    "    very_best_cat_features,\n",
    "    repeat_times)\n",
    "random_forest_poly = poly_random_forest_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    very_best_num_features,\n",
    "    very_best_cat_features,\n",
    "    repeat_times)\n",
    "gradient_boosting_poly = poly_gradient_boosting_builder(\n",
    "    churn_data_frame,\n",
    "    churn_labels_frame,\n",
    "    very_best_num_features,\n",
    "    very_best_cat_features,\n",
    "    repeat_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\"Ridge Balanced\",\"Ridge Poly\", \"Random Forest Balanced\", \"Random Forest Poly\", \"Gradient boosting balanced\", \"Gradient boosting Poly\"]\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        calculate_metrics(ridge_balanced, predict_ridge_proba),\n",
    "        calculate_metrics(ridge_poly, predict_ridge_proba),\n",
    "        calculate_metrics(random_forest_balanced, predict_model_proba),\n",
    "        calculate_metrics(random_forest_poly, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_balanced, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_poly, predict_model_proba)\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Precision\", \"Recall\", \"F-Score\", \"Log Loss\", \"Roc Auc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "Строим GradientBoostingClassifier, добавляя по одному вещественные признаки, которые повышают качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724014191032 Index(['Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var13',\n",
      "       'Var14', 'Var16', 'Var17', 'Var25', 'Var27', 'Var28', 'Var29', 'Var34',\n",
      "       'Var49', 'Var51', 'Var53', 'Var58', 'Var61', 'Var73', 'Var74', 'Var81',\n",
      "       'Var82', 'Var83', 'Var88', 'Var90', 'Var99', 'Var110', 'Var113',\n",
      "       'Var116', 'Var126', 'Var127', 'Var140', 'Var142', 'Var147', 'Var157',\n",
      "       'Var161', 'Var172', 'Var186', 'Var187', 'Var189', 'Var190'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "repeat_times=12\n",
    "num_cols_selected = pd.Index([])\n",
    "best_metric = 0\n",
    "best_model = None\n",
    "best_numeric_features = pd.Index([])\n",
    "\n",
    "for col in numeric_columns:\n",
    "    num_cols_selected = num_cols_selected.insert(len(num_cols_selected), col)\n",
    "    gb_selected_num = balanced_gradient_boosting_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        num_cols_selected,\n",
    "        pd.Index([]),\n",
    "        repeat_times)\n",
    "    \n",
    "    auc_roc = gb_selected_num[2]\n",
    "    if np.isin(col, gb_selected_num[5][0], invert=True) and np.round(auc_roc, 5) > np.round(best_metric, 5):\n",
    "        best_metric = auc_roc\n",
    "        best_model = gb_selected_num\n",
    "        best_numeric_features = num_cols_selected.copy()\n",
    "    else:\n",
    "        num_cols_selected = num_cols_selected.drop(col)\n",
    "        \n",
    "print (best_metric, best_numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также по-одному добавим категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739212100143 Index(['Var191', 'Var193', 'Var194', 'Var196', 'Var201', 'Var205', 'Var208',\n",
      "       'Var211', 'Var212', 'Var213', 'Var215', 'Var217', 'Var218', 'Var220',\n",
      "       'Var221', 'Var229'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cat_cols_selected = pd.Index([])\n",
    "best_cat_metric = 0\n",
    "best_cat_model = None\n",
    "best_cat_features = pd.Index([])\n",
    "\n",
    "for col in categorial_columns:\n",
    "    cat_cols_selected = cat_cols_selected.insert(len(cat_cols_selected), col)\n",
    "    gb_selected_cat = balanced_gradient_boosting_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        best_numeric_features,\n",
    "        cat_cols_selected,\n",
    "        repeat_times)\n",
    "    \n",
    "    auc_roc = gb_selected_cat[2]\n",
    "    if np.isin(col, gb_selected_cat[5][1], invert=True) and np.round(auc_roc, 5) > np.round(best_cat_metric, 5):\n",
    "        best_cat_metric = auc_roc\n",
    "        best_cat_model = gb_selected_cat\n",
    "        best_cat_features = cat_cols_selected.copy()\n",
    "    else:\n",
    "        cat_cols_selected = cat_cols_selected.drop(col)\n",
    "        \n",
    "print (best_cat_metric, best_cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим GradientBoostingClassifier с отобранными категориальными и вещественными признаками, удаляем те вещественные признаки, которые уменьшают качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741112422624 Index(['Var90', 'Var99', 'Var110', 'Var113', 'Var116', 'Var126', 'Var127',\n",
      "       'Var140', 'Var142', 'Var147', 'Var157', 'Var161', 'Var172', 'Var186',\n",
      "       'Var187', 'Var189', 'Var190', 'Var3', 'Var4', 'Var5', 'Var7', 'Var13',\n",
      "       'Var14', 'Var16', 'Var17', 'Var25', 'Var28', 'Var29', 'Var34', 'Var49',\n",
      "       'Var51', 'Var53', 'Var58', 'Var61', 'Var73', 'Var81', 'Var82', 'Var83'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "num_cols_selected = best_numeric_features.copy()\n",
    "very_best_metric = best_metric\n",
    "very_best_model = best_model\n",
    "very_best_num_features = best_numeric_features.copy()\n",
    "\n",
    "for col in best_numeric_features:\n",
    "    num_cols_selected = num_cols_selected.drop(col)\n",
    "    gb_selected_num = balanced_gradient_boosting_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        num_cols_selected,\n",
    "        best_cat_features,\n",
    "        repeat_times)\n",
    "    \n",
    "    auc_roc = gb_selected_num[2]\n",
    "    if np.round(auc_roc, 5) >= np.round(very_best_metric, 5):\n",
    "        very_best_metric = auc_roc\n",
    "        very_best_model = gb_selected_num\n",
    "        very_best_num_features = num_cols_selected.copy()\n",
    "    else:\n",
    "        num_cols_selected = num_cols_selected.insert(len(num_cols_selected), col)\n",
    "        \n",
    "print (very_best_metric, very_best_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим модель, удаляя те категориальные признаки, которые понижают качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741840723884 Index(['Var221', 'Var229', 'Var191', 'Var193', 'Var194', 'Var196', 'Var201',\n",
      "       'Var205', 'Var208', 'Var211', 'Var212', 'Var213', 'Var215', 'Var217',\n",
      "       'Var218'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cat_cols_selected = best_cat_features.copy()\n",
    "very_best_cat_metric = very_best_metric\n",
    "very_best_cat_model = very_best_model\n",
    "very_best_cat_features = best_cat_features.copy()\n",
    "\n",
    "for col in best_cat_features:\n",
    "    cat_cols_selected = cat_cols_selected.drop(col)\n",
    "    gb_selected_cat = balanced_gradient_boosting_builder(\n",
    "        churn_data_frame,\n",
    "        churn_labels_frame,\n",
    "        very_best_num_features,\n",
    "        cat_cols_selected,\n",
    "        repeat_times)\n",
    "    auc_roc = gb_selected_cat[2]\n",
    "    if np.round(auc_roc, 5) >= np.round(very_best_cat_metric, 5):\n",
    "        very_best_cat_metric = auc_roc\n",
    "        very_best_cat_model = gb_selected_cat\n",
    "        very_best_cat_features = cat_cols_selected.copy()\n",
    "    else:\n",
    "        cat_cols_selected = cat_cols_selected.insert(len(cat_cols_selected), col)\n",
    "        \n",
    "print (very_best_cat_metric, very_best_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.140625,\n",
       " 0.63544668587896258,\n",
       " 0.23028720626631854,\n",
       " 0.56711936525031048,\n",
       " 0.74184072388407762)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(very_best_cat_model, predict_model_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что в kaggle датасете есть пара колонок (Var209 и Var230) в которых совсем нет данных. Удалим их, т.к. они не сыграют роли при построении вероятностей для kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_contain_lots_new_cats = [\n",
    "#    \"Var200\", \"Var214\", \"Var217\", \"Var199\", \"Var198\",\n",
    "#    \"Var220\", \"Var222\", \"Var202\", \"Var216\"]\n",
    "#columns_not_contain_data = [\"Var209\", \"Var230\"]\n",
    "#num_columns_to_remove = [\"Var20\", \"Var79\"]\n",
    "#numeric_columns_cleaned = very_best_num_features.drop(num_columns_to_remove)\n",
    "#categorial_columns_no_new_cats = very_best_cat_features #.drop(columns_not_contain_data)\n",
    "all_best_features = pd.Index(list(very_best_num_features) + list(very_best_cat_features))\n",
    "frame_best_features = churn_data_frame[all_best_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим ridge, random forest и gradient boosting классификаторы основываясь на отборе признаков проведенном выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_add_del = balanced_ridge_builder(\n",
    "    frame_best_features,\n",
    "    churn_labels_frame,\n",
    "    very_best_num_features,\n",
    "    very_best_cat_features,\n",
    "    repeat_times)\n",
    "random_forest_add_del = balanced_random_forest_builder(\n",
    "    frame_best_features,\n",
    "    churn_labels_frame,\n",
    "    very_best_num_features,\n",
    "    very_best_cat_features,\n",
    "    repeat_times)\n",
    "gradient_boosting_add_del = balanced_gradient_boosting_builder(\n",
    "    frame_best_features,\n",
    "    churn_labels_frame,\n",
    "    very_best_num_features,\n",
    "    very_best_cat_features,\n",
    "    repeat_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Mean      Best\n",
      "Ridge poly             0.655119  0.657426\n",
      "Ridge add del          0.674195  0.685113\n",
      "Random forest poly     0.636211  0.642317\n",
      "Random forest add del  0.639326  0.645680\n",
      "GB Poly                0.721617  0.729056\n",
      "GB Add del             0.727653  0.741635\n",
      "\n",
      "                       Precision    Recall   F-Score  Log Loss   Roc Auc\n",
      "Ridge poly              0.160870  0.106628  0.128250  0.672250  0.657426\n",
      "Ridge add del           0.136850  0.515850  0.216314  0.615196  0.685113\n",
      "Random forest poly      0.133333  0.017266  0.030573  0.827347  0.642317\n",
      "Random forest add del   0.186667  0.020144  0.036364  0.840164  0.645680\n",
      "GB Poly                 0.139930  0.629683  0.228976  0.561926  0.729056\n",
      "GB Add del              0.140625  0.635447  0.230287  0.567213  0.741635\n"
     ]
    }
   ],
   "source": [
    "indices = [\"Ridge poly\", \"Ridge add del\",\"Random forest poly\", \"Random forest add del\",\n",
    "           \"GB Poly\", \"GB Add del\"]\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        [ridge_poly[0], ridge_poly[2]],\n",
    "        [ridge_add_del[0], ridge_add_del[2]],\n",
    "        [random_forest_poly[0], random_forest_poly[2]],\n",
    "        [random_forest_add_del[0], random_forest_add_del[2]],\n",
    "        [gradient_boosting_poly[0], gradient_boosting_poly[2]],\n",
    "        [gradient_boosting_add_del[0], gradient_boosting_add_del[2]]\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Mean\", \"Best\"]))\n",
    "print()\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        calculate_metrics(ridge_poly, predict_ridge_proba),\n",
    "        calculate_metrics(ridge_add_del, predict_ridge_proba),\n",
    "        calculate_metrics(random_forest_poly, predict_model_proba),\n",
    "        calculate_metrics(random_forest_add_del, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_poly, predict_model_proba),\n",
    "        calculate_metrics(gradient_boosting_add_del, predict_model_proba)\n",
    "    ],\n",
    "    index=indices,\n",
    "    columns=[\"Precision\", \"Recall\", \"F-Score\", \"Log Loss\", \"Roc Auc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что на удалось чуть-чуть улучшить модели за счет отбора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение решения для kaggle\n",
    "\n",
    "Ячейка ниже использовалась для поиска новых категорий в test dataset и поиска пустых колонок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['Var217', 'Var212']\n"
     ]
    }
   ],
   "source": [
    "# Посмотреть насколько больше категорий стало\n",
    "cols_to_remove = []\n",
    "print(kaggle_data_frame.shape[0])\n",
    "for col in very_best_cat_features:\n",
    "    full_train_col = set(churn_data_frame[col].as_matrix())\n",
    "    kaggle_col = set(kaggle_data_frame[col].as_matrix())\n",
    "    intersected = (kaggle_col & full_train_col)\n",
    "    for v in intersected:\n",
    "        kaggle_col.remove(v)\n",
    "    cols_to_remove.append((col, len(kaggle_col)))\n",
    "print (list(ctr[0] for ctr in sorted(cols_to_remove, key=lambda ctr: -ctr[1]) if ctr[1] > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим новые категории (которых очень много) самыми частыми по колонкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = []\n",
    "cleaned_kaggle_data_frame = kaggle_data_frame.copy()\n",
    "for col in very_best_cat_features:\n",
    "    column = cleaned_kaggle_data_frame[col]\n",
    "    train_column = churn_data_frame[col]\n",
    "    full_train_col = set(train_column.as_matrix())\n",
    "    kaggle_col = set(column.as_matrix())\n",
    "    intersected = list(kaggle_col & full_train_col)\n",
    "    for v in intersected:\n",
    "        kaggle_col.remove(v)\n",
    "    frequencies = train_column.value_counts()\n",
    "    if (len (frequencies) < 1):\n",
    "        cols_to_drop.append(col)\n",
    "        continue\n",
    "    most_frequent_value = frequencies.index[0]\n",
    "    for to_remove in kaggle_col:\n",
    "        column = column.replace(to_remove, most_frequent_value)\n",
    "    cleaned_kaggle_data_frame[col] = column\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим теже преобразования, какие мы применяли для обучения тренировочной модели к kaggle датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very_best_cat_model very_best_cat_features very_best_num_features\n",
    "kaggle_numeric = cleaned_kaggle_data_frame[very_best_num_features].copy()\n",
    "kaggle_categorial = cleaned_kaggle_data_frame[very_best_cat_features].copy()\n",
    "\n",
    "numeric_encoder, categorial_encoder = very_best_cat_model[4]\n",
    "k_t_n, kaggle_num_means, kaggle_dropped_columns = fill_numericna_means(kaggle_numeric, kaggle_numeric)\n",
    "kaggle_means_scaled = numeric_encoder.transform(kaggle_num_means)\n",
    "\n",
    "kaggle_frame_nav = fill_na_frequent_values(kaggle_categorial)\n",
    "kaggle_encoded = categorial_encoder.transform(kaggle_frame_nav.as_matrix())\n",
    "\n",
    "X_kaggle = hstack([\n",
    "    coo_matrix(kaggle_means_scaled),\n",
    "    kaggle_encoded\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вероятности для Kaggle и сохраним их в отдельный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.533128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.293048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     result\n",
       "0  0.315521\n",
       "1  0.533128\n",
       "2  0.246412\n",
       "3  0.293048\n",
       "4  0.166113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_predict_probabilities = predict_ridge_proba(X_kaggle, very_best_cat_model[1])\n",
    "kg_predict_frame = pd.DataFrame(kg_predict_probabilities, columns=[\"result\"])\n",
    "kg_predict_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_predict_frame.to_csv(\"..\\..\\Results\\churn_kaggle_gb_add_del.csv\", index=True, index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "В этой модели мы применили\n",
    "- label encoding и one-hot encoding для обработки категориальных признаков\n",
    "- scaling для обработки вещественных признаков\n",
    "- сэмплирование для балансировки классов\n",
    "- отбор признаков по методу add-del\n",
    "\n",
    "Нам надо было сделать решение, которое превосходило бы базовое решение на kaggle.\n",
    "На kaggle есть два лидер борда (private и public) и там чуть-чуть разные оценки\n",
    "Private: 0.68981\n",
    "Public: 0.65318\n",
    "\n",
    "Моя модель на kaggle показала следующие результаты (см. скриншот)\n",
    "Private: 0.71212\n",
    "Public: 0.66762\n",
    "\n",
    "Таким образом можно утверждать, что я улучшил базовое решение.\n",
    "\n",
    "P.S. К сожалению соревнование уже закончено и я так и не смог попасть в LeaderBoard, но я приложил все скриншоты, на которых можно посмотреть мою оценку на kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполнение вещественных пропусков (нулями, медианами)\n",
    "# Заполнение категориальных пропусков (частотами)\n",
    "# Полиномиальные фичи"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
