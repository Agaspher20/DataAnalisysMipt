{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Неделя 3. Соревнование.\n",
    "В этом задании вам нужно воспользоваться опытом предыдущих недель, чтобы побить бейзлайн в [соревновании по сентимент-анализу отзывов](https://www.kaggle.com/c/product-reviews-sentiment-analysis-light) на товары на Kaggle Inclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8269"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed = random.randint(0, 10000)\n",
    "seed = 8269\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0          2 . take around 10,000 640x480 pictures .      1\n",
       "1  i downloaded a trial version of computer assoc...      1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
       "3  i dont especially like how music files are uns...      0\n",
       "4  i was using the cheapie pail ... and it worked...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\n",
    "    \"..\\..\\Data\\products_sentiment_train.tsv\",\n",
    "    \"\\t\",\n",
    "    names=[\"text\", \"class\"],\n",
    "    dtype={ \"text\": \"str\", \"class\": \"int\" })\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so , why the small digital elph , rather than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/4 way through the first disk we played on it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better for the zen micro is outlook compatibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 . play gameboy color games on it with goboy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>likewise , i 've heard norton 2004 professiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "Id                                                   \n",
       "0   so , why the small digital elph , rather than ...\n",
       "1   3/4 way through the first disk we played on it...\n",
       "2   better for the zen micro is outlook compatibil...\n",
       "3     6 . play gameboy color games on it with goboy .\n",
       "4   likewise , i 've heard norton 2004 professiona..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\n",
    "    \"..\\..\\Data\\products_sentiment_test.tsv\",\n",
    "    \"\\t\",\n",
    "    index_col=\"Id\",\n",
    "    dtype={ \"text\": \"str\", \"Id\": \"int\" })\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_data[\"text\"].tolist()\n",
    "classes = train_data[\"class\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model_template(\n",
    "    model_name,\n",
    "    model_pipe,\n",
    "    frame,\n",
    "    frame_columns,\n",
    "    score_texts,\n",
    "    score_classes,\n",
    "    print_results=True\n",
    "):\n",
    "    accuracy_scores = cross_val_score(\n",
    "        model_pipe,\n",
    "        score_texts,\n",
    "        score_classes,\n",
    "        scoring=\"accuracy\")\n",
    "    roc_auc_scores = cross_val_score(\n",
    "        model_pipe,\n",
    "        score_texts,\n",
    "        score_classes,\n",
    "        scoring=\"roc_auc\")\n",
    "    average_accuracy = np.average(accuracy_scores)\n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    average_roc_auc = np.average(roc_auc_scores)\n",
    "    std_roc_auc = np.std(roc_auc_scores)\n",
    "    frame = frame.append(pd.DataFrame([\n",
    "        [average_accuracy, std_accuracy, average_roc_auc, std_roc_auc]\n",
    "    ], index=[model_name], columns=frame_columns))\n",
    "    if(print_results):\n",
    "        print (\"Accuracy:\\n\\tAverage: {0:.3f}\\n\\tStandard Deviation: {1:.3f}\".format(\n",
    "            average_accuracy,\n",
    "            std_accuracy))\n",
    "        print (\"ROC AUC:\\n\\tAverage: {0:.3f}\\n\\tStandard Deviation: {1:.3f}\".format(\n",
    "            np.average(roc_auc_scores),\n",
    "            np.std(roc_auc_scores)))\n",
    "    return frame\n",
    "\n",
    "# Построить ROC AUC кривую\n",
    "def ROC_AUC_curve(vectorizer_factory, model_factory):\n",
    "    rate = 0.3\n",
    "    vectorizer = vectorizer_factory()\n",
    "    vectorized_texts = vectorizer.fit_transform(texts)\n",
    "    train_texts, test_texts, train_classes, test_classes = train_test_split(\n",
    "        vectorized_texts,\n",
    "        classes,\n",
    "        test_size=rate,\n",
    "        random_state=seed,\n",
    "        stratify=classes\n",
    "    )\n",
    "    model = model_factory().fit(train_texts, train_classes)\n",
    "    probabilities = list(map(lambda pred: pred[1], model.predict_proba(test_texts)))\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(test_classes, probabilities)\n",
    "    distance,fpr_v,tpr_v,thr_v = min(zip(np.sqrt((1.-tpr)**2+fpr**2),fpr,tpr,thr), key=lambda d:d[0])\n",
    "\n",
    "    plt.plot(fpr, tpr, label=\"ROC AUC curve\")\n",
    "    plt.scatter(fpr_v, tpr_v, color=\"red\")\n",
    "    plt.xlabel(\"false positive rate\")\n",
    "    plt.ylabel(\"true positive rate\")\n",
    "    plt.title(\"ROC AUC\")\n",
    "    return distance\n",
    "\n",
    "def model_metrics(vectorizer_factory, model_factory):\n",
    "    vectorizer = vectorizer_factory()\n",
    "    vectorized_texts = vectorizer.fit_transform(texts)\n",
    "    full_model = model_factory().fit(vectorized_texts, classes)\n",
    "    \n",
    "    distance = ROC_AUC_curve(vectorizer_factory, model_factory)\n",
    "    \n",
    "    main_features = list(map(\n",
    "        lambda feature: feature[0],\n",
    "        sorted(\n",
    "            zip(vectorizer.get_feature_names(), np.abs(full_model.coef_[0])),\n",
    "            key=lambda feature: feature[1],\n",
    "            reverse=True)[:5]))\n",
    "    print (\"Main features:\", main_features)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>ROC_AUC_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,6) logistic</th>\n",
       "      <td>0.837519</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.923966</td>\n",
       "      <td>0.008160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized TF-IDF logistic. C=0.8</th>\n",
       "      <td>0.836734</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.922593</td>\n",
       "      <td>0.008359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,5) logistic</th>\n",
       "      <td>0.836348</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.922453</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.897232</td>\n",
       "      <td>0.009240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords logistic</th>\n",
       "      <td>0.812407</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF Logistic baseline</th>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.894245</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords logistic</th>\n",
       "      <td>0.804168</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.887977</td>\n",
       "      <td>0.005197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords logistic</th>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords logistic</th>\n",
       "      <td>0.791995</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.878878</td>\n",
       "      <td>0.004080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing NLTK English stopwords logistic</th>\n",
       "      <td>0.782572</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.870553</td>\n",
       "      <td>0.006265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Sklearn English stopwords logistic</th>\n",
       "      <td>0.774733</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.865521</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Logistic baseline</th>\n",
       "      <td>0.772374</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.865579</td>\n",
       "      <td>0.006791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Accuracy  Accuracy_std   ROC_AUC  \\\n",
       "Id                                                                             \n",
       "Tfidf N-grams (1,6) logistic                0.837519      0.000181  0.923966   \n",
       "Regularized TF-IDF logistic. C=0.8          0.836734      0.000664  0.922593   \n",
       "Count N-grams (1,5) logistic                0.836348      0.009070  0.922453   \n",
       "Count Logistic                              0.817895      0.010648  0.897232   \n",
       "Count NLTK English stopwords logistic       0.812407      0.008775  0.891127   \n",
       "TF-IDF Logistic baseline                    0.805341      0.008842  0.894245   \n",
       "Count Sklearn English stopwords logistic    0.804168      0.009856  0.887977   \n",
       "Tfidf NLTK English stopwords logistic       0.801015      0.008513  0.884795   \n",
       "Tfidf Sklearn English stopwords logistic    0.791995      0.002666  0.878878   \n",
       "Hashing NLTK English stopwords logistic     0.782572      0.008452  0.870553   \n",
       "Hashing Sklearn English stopwords logistic  0.774733      0.008527  0.865521   \n",
       "Hashing Logistic baseline                   0.772374      0.007089  0.865579   \n",
       "\n",
       "                                            ROC_AUC_std  \n",
       "Id                                                       \n",
       "Tfidf N-grams (1,6) logistic                   0.008160  \n",
       "Regularized TF-IDF logistic. C=0.8             0.008359  \n",
       "Count N-grams (1,5) logistic                   0.006407  \n",
       "Count Logistic                                 0.009240  \n",
       "Count NLTK English stopwords logistic          0.005639  \n",
       "TF-IDF Logistic baseline                       0.005339  \n",
       "Count Sklearn English stopwords logistic       0.005197  \n",
       "Tfidf NLTK English stopwords logistic          0.005838  \n",
       "Tfidf Sklearn English stopwords logistic       0.004080  \n",
       "Hashing NLTK English stopwords logistic        0.006265  \n",
       "Hashing Sklearn English stopwords logistic     0.005346  \n",
       "Hashing Logistic baseline                      0.006791  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_columns = [\"Accuracy\", \"Accuracy_std\", \"ROC_AUC\", \"ROC_AUC_std\"]\n",
    "# results = pd.DataFrame(columns=result_columns)\n",
    "results = pd.read_csv(\n",
    "    \"..\\..\\Results\\sentiment_kaggle_results_table.csv\",\n",
    "    \",\",\n",
    "    index_col=\"Id\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data balancing (Oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(input_texts, input_classes, diff_size = 548):\n",
    "    negative_texts, negative_classes = zip(*filter(lambda row: row[1] == 0, zip(input_texts, input_classes)))\n",
    "    negative_texts_list = []\n",
    "    random.seed(seed)\n",
    "    for i in range(0, diff_size):\n",
    "        negative_texts_list = negative_texts_list + [negative_texts[random.randint(0, len(negative_texts)- 1)]]\n",
    "        \n",
    "    balanced_texts = input_texts + negative_texts_list\n",
    "    balanced_classes = input_classes + ([0] * diff_size)\n",
    "    balanced_data = list(zip(balanced_texts, balanced_classes))\n",
    "    shuffle(balanced_data)\n",
    "    return zip(*balanced_data)\n",
    "\n",
    "def score_model_oversampling(\n",
    "    model_name,\n",
    "    model_pipe,\n",
    "    result_frame,\n",
    "    result_frame_columns,\n",
    "    print_results=True\n",
    "):\n",
    "    balanced_texts, balanced_classes = balance_data(texts, classes)\n",
    "    return score_model_template(\n",
    "        model_name,\n",
    "        model_pipe,\n",
    "        result_frame,\n",
    "        result_frame_columns,\n",
    "        balanced_texts,\n",
    "        balanced_classes,\n",
    "        print_results\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.799\n",
      "\tStandard Deviation: 0.003\n",
      "ROC AUC:\n",
      "\tAverage: 0.885\n",
      "\tStandard Deviation: 0.006\n"
     ]
    }
   ],
   "source": [
    "count_pipe = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer()),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))])\n",
    "results = score_model_oversampling(\n",
    "    \"Count Random Forest\",\n",
    "    count_pipe,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.779\n",
      "\tStandard Deviation: 0.012\n",
      "ROC AUC:\n",
      "\tAverage: 0.872\n",
      "\tStandard Deviation: 0.020\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words(\"english\")\n",
    "nltk_stopwords_count = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer(stop_words=english_stopwords)),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Count NLTK English stopwords random forest\",\n",
    "    nltk_stopwords_count,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.786\n",
      "\tStandard Deviation: 0.009\n",
      "ROC AUC:\n",
      "\tAverage: 0.875\n",
      "\tStandard Deviation: 0.006\n"
     ]
    }
   ],
   "source": [
    "sklearn_stopwords_count = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer(stop_words=\"english\")),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Count Sklearn English stopwords random forest\",\n",
    "    sklearn_stopwords_count,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>ROC_AUC_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,6) logistic</th>\n",
       "      <td>0.837519</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.923966</td>\n",
       "      <td>0.008160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized TF-IDF logistic. C=0.8</th>\n",
       "      <td>0.836734</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.922593</td>\n",
       "      <td>0.008359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,5) logistic</th>\n",
       "      <td>0.836348</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.922453</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.897232</td>\n",
       "      <td>0.009240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords logistic</th>\n",
       "      <td>0.812407</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF Logistic baseline</th>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.894245</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords logistic</th>\n",
       "      <td>0.804168</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.887977</td>\n",
       "      <td>0.005197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords logistic</th>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords logistic</th>\n",
       "      <td>0.791995</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.878878</td>\n",
       "      <td>0.004080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing NLTK English stopwords logistic</th>\n",
       "      <td>0.782572</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.870553</td>\n",
       "      <td>0.006265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Sklearn English stopwords logistic</th>\n",
       "      <td>0.774733</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.865521</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Logistic baseline</th>\n",
       "      <td>0.772374</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.865579</td>\n",
       "      <td>0.006791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Random Forest</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords random forest</th>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.872445</td>\n",
       "      <td>0.019879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords random forest</th>\n",
       "      <td>0.786492</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.874832</td>\n",
       "      <td>0.005678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,2) random forest</th>\n",
       "      <td>0.807706</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.884365</td>\n",
       "      <td>0.011908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,3) random forest</th>\n",
       "      <td>0.797485</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.877929</td>\n",
       "      <td>0.010537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,4) random forest</th>\n",
       "      <td>0.802970</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>0.004425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,5) random forest</th>\n",
       "      <td>0.799449</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0.879437</td>\n",
       "      <td>0.001155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,6) random forest</th>\n",
       "      <td>0.807705</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.878187</td>\n",
       "      <td>0.006531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,7) random forest</th>\n",
       "      <td>0.797886</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.870790</td>\n",
       "      <td>0.013831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,8) random forest</th>\n",
       "      <td>0.817899</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.883957</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,9) random forest</th>\n",
       "      <td>0.806126</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.880236</td>\n",
       "      <td>0.014437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,10) random forest</th>\n",
       "      <td>0.804546</td>\n",
       "      <td>0.013445</td>\n",
       "      <td>0.879020</td>\n",
       "      <td>0.008068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,11) random forest</th>\n",
       "      <td>0.813575</td>\n",
       "      <td>0.011332</td>\n",
       "      <td>0.890302</td>\n",
       "      <td>0.004685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,12) random forest</th>\n",
       "      <td>0.808488</td>\n",
       "      <td>0.014575</td>\n",
       "      <td>0.878265</td>\n",
       "      <td>0.016865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,13) random forest</th>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>0.877027</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,14) random forest</th>\n",
       "      <td>0.807691</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.879713</td>\n",
       "      <td>0.002719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,15) random forest</th>\n",
       "      <td>0.793163</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.863306</td>\n",
       "      <td>0.003138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,16) random forest</th>\n",
       "      <td>0.808872</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.877253</td>\n",
       "      <td>0.006108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,17) random forest</th>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.026276</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.016375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,18) random forest</th>\n",
       "      <td>0.797481</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.872522</td>\n",
       "      <td>0.008062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,19) random forest</th>\n",
       "      <td>0.808467</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.879263</td>\n",
       "      <td>0.008502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,20) random forest</th>\n",
       "      <td>0.796727</td>\n",
       "      <td>0.023044</td>\n",
       "      <td>0.870541</td>\n",
       "      <td>0.023624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,21) random forest</th>\n",
       "      <td>0.803769</td>\n",
       "      <td>0.006353</td>\n",
       "      <td>0.875466</td>\n",
       "      <td>0.016562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,22) random forest</th>\n",
       "      <td>0.815157</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.010386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,23) random forest</th>\n",
       "      <td>0.796693</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.877206</td>\n",
       "      <td>0.014598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,24) random forest</th>\n",
       "      <td>0.813968</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>0.874672</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,25) random forest</th>\n",
       "      <td>0.808078</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.882690</td>\n",
       "      <td>0.003596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,26) random forest</th>\n",
       "      <td>0.807702</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.874658</td>\n",
       "      <td>0.011617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,27) random forest</th>\n",
       "      <td>0.803372</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.877335</td>\n",
       "      <td>0.005656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,28) random forest</th>\n",
       "      <td>0.808082</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.879239</td>\n",
       "      <td>0.003111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,29) random forest</th>\n",
       "      <td>0.802605</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.874219</td>\n",
       "      <td>0.012610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  Accuracy_std  \\\n",
       "Tfidf N-grams (1,6) logistic                   0.837519      0.000181   \n",
       "Regularized TF-IDF logistic. C=0.8             0.836734      0.000664   \n",
       "Count N-grams (1,5) logistic                   0.836348      0.009070   \n",
       "Count Logistic                                 0.817895      0.010648   \n",
       "Count NLTK English stopwords logistic          0.812407      0.008775   \n",
       "TF-IDF Logistic baseline                       0.805341      0.008842   \n",
       "Count Sklearn English stopwords logistic       0.804168      0.009856   \n",
       "Tfidf NLTK English stopwords logistic          0.801015      0.008513   \n",
       "Tfidf Sklearn English stopwords logistic       0.791995      0.002666   \n",
       "Hashing NLTK English stopwords logistic        0.782572      0.008452   \n",
       "Hashing Sklearn English stopwords logistic     0.774733      0.008527   \n",
       "Hashing Logistic baseline                      0.772374      0.007089   \n",
       "Count Logistic                                 0.799059      0.002669   \n",
       "Count Random Forest                            0.799059      0.002669   \n",
       "Count NLTK English stopwords random forest     0.779434      0.011555   \n",
       "Count Sklearn English stopwords random forest  0.786492      0.009104   \n",
       "Count N-grams (1,2) random forest              0.807706      0.013449   \n",
       "Count N-grams (1,3) random forest              0.797485      0.011378   \n",
       "Count N-grams (1,4) random forest              0.802970      0.012083   \n",
       "Count N-grams (1,5) random forest              0.799449      0.002892   \n",
       "Count N-grams (1,6) random forest              0.807705      0.011817   \n",
       "Count N-grams (1,7) random forest              0.797886      0.005615   \n",
       "Count N-grams (1,8) random forest              0.817899      0.003519   \n",
       "Count N-grams (1,9) random forest              0.806126      0.020847   \n",
       "Count N-grams (1,10) random forest             0.804546      0.013445   \n",
       "Count N-grams (1,11) random forest             0.813575      0.011332   \n",
       "Count N-grams (1,12) random forest             0.808488      0.014575   \n",
       "Count N-grams (1,13) random forest             0.800232      0.004159   \n",
       "Count N-grams (1,14) random forest             0.807691      0.008679   \n",
       "Count N-grams (1,15) random forest             0.793163      0.007189   \n",
       "Count N-grams (1,16) random forest             0.808872      0.004657   \n",
       "Count N-grams (1,17) random forest             0.805755      0.026276   \n",
       "Count N-grams (1,18) random forest             0.797481      0.008399   \n",
       "Count N-grams (1,19) random forest             0.808467      0.012021   \n",
       "Count N-grams (1,20) random forest             0.796727      0.023044   \n",
       "Count N-grams (1,21) random forest             0.803769      0.006353   \n",
       "Count N-grams (1,22) random forest             0.815157      0.007309   \n",
       "Count N-grams (1,23) random forest             0.796693      0.013629   \n",
       "Count N-grams (1,24) random forest             0.813968      0.004564   \n",
       "Count N-grams (1,25) random forest             0.808078      0.010160   \n",
       "Count N-grams (1,26) random forest             0.807702      0.009920   \n",
       "Count N-grams (1,27) random forest             0.803372      0.002760   \n",
       "Count N-grams (1,28) random forest             0.808082      0.005103   \n",
       "Count N-grams (1,29) random forest             0.802605      0.015748   \n",
       "\n",
       "                                                ROC_AUC  ROC_AUC_std  \n",
       "Tfidf N-grams (1,6) logistic                   0.923966     0.008160  \n",
       "Regularized TF-IDF logistic. C=0.8             0.922593     0.008359  \n",
       "Count N-grams (1,5) logistic                   0.922453     0.006407  \n",
       "Count Logistic                                 0.897232     0.009240  \n",
       "Count NLTK English stopwords logistic          0.891127     0.005639  \n",
       "TF-IDF Logistic baseline                       0.894245     0.005339  \n",
       "Count Sklearn English stopwords logistic       0.887977     0.005197  \n",
       "Tfidf NLTK English stopwords logistic          0.884795     0.005838  \n",
       "Tfidf Sklearn English stopwords logistic       0.878878     0.004080  \n",
       "Hashing NLTK English stopwords logistic        0.870553     0.006265  \n",
       "Hashing Sklearn English stopwords logistic     0.865521     0.005346  \n",
       "Hashing Logistic baseline                      0.865579     0.006791  \n",
       "Count Logistic                                 0.884808     0.006048  \n",
       "Count Random Forest                            0.884808     0.006048  \n",
       "Count NLTK English stopwords random forest     0.872445     0.019879  \n",
       "Count Sklearn English stopwords random forest  0.874832     0.005678  \n",
       "Count N-grams (1,2) random forest              0.884365     0.011908  \n",
       "Count N-grams (1,3) random forest              0.877929     0.010537  \n",
       "Count N-grams (1,4) random forest              0.873840     0.004425  \n",
       "Count N-grams (1,5) random forest              0.879437     0.001155  \n",
       "Count N-grams (1,6) random forest              0.878187     0.006531  \n",
       "Count N-grams (1,7) random forest              0.870790     0.013831  \n",
       "Count N-grams (1,8) random forest              0.883957     0.004047  \n",
       "Count N-grams (1,9) random forest              0.880236     0.014437  \n",
       "Count N-grams (1,10) random forest             0.879020     0.008068  \n",
       "Count N-grams (1,11) random forest             0.890302     0.004685  \n",
       "Count N-grams (1,12) random forest             0.878265     0.016865  \n",
       "Count N-grams (1,13) random forest             0.877027     0.007163  \n",
       "Count N-grams (1,14) random forest             0.879713     0.002719  \n",
       "Count N-grams (1,15) random forest             0.863306     0.003138  \n",
       "Count N-grams (1,16) random forest             0.877253     0.006108  \n",
       "Count N-grams (1,17) random forest             0.878282     0.016375  \n",
       "Count N-grams (1,18) random forest             0.872522     0.008062  \n",
       "Count N-grams (1,19) random forest             0.879263     0.008502  \n",
       "Count N-grams (1,20) random forest             0.870541     0.023624  \n",
       "Count N-grams (1,21) random forest             0.875466     0.016562  \n",
       "Count N-grams (1,22) random forest             0.879857     0.010386  \n",
       "Count N-grams (1,23) random forest             0.877206     0.014598  \n",
       "Count N-grams (1,24) random forest             0.874672     0.002506  \n",
       "Count N-grams (1,25) random forest             0.882690     0.003596  \n",
       "Count N-grams (1,26) random forest             0.874658     0.011617  \n",
       "Count N-grams (1,27) random forest             0.877335     0.005656  \n",
       "Count N-grams (1,28) random forest             0.879239     0.003111  \n",
       "Count N-grams (1,29) random forest             0.874219     0.012610  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ngram = results\n",
    "for max_ngram in range(2, 30):\n",
    "    sklearn_count_ngram = Pipeline([\n",
    "        (\"vectorize\", CountVectorizer(analyzer=\"word\", ngram_range=(1, max_ngram))),\n",
    "        (\"model\", RandomForestClassifier(random_state=seed))\n",
    "    ])\n",
    "    results_ngram = score_model_oversampling(\n",
    "        \"Count N-grams (1,{0}) random forest\".format(max_ngram),\n",
    "        sklearn_count_ngram,\n",
    "        results_ngram,\n",
    "        result_columns,\n",
    "        False\n",
    "    )\n",
    "results_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.818\n",
      "\tStandard Deviation: 0.004\n",
      "ROC AUC:\n",
      "\tAverage: 0.884\n",
      "\tStandard Deviation: 0.004\n"
     ]
    }
   ],
   "source": [
    "sklearn_count_ngram_1_8 = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer(analyzer=\"word\", ngram_range=(1, 8))),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Count N-grams (1,8) random forest\",\n",
    "    sklearn_count_ngram_1_8,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.782\n",
      "\tStandard Deviation: 0.005\n",
      "ROC AUC:\n",
      "\tAverage: 0.876\n",
      "\tStandard Deviation: 0.005\n"
     ]
    }
   ],
   "source": [
    "tfidf_pipe = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer()),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))])\n",
    "results = score_model_oversampling(\n",
    "    \"TF-IDF random forest baseline\",\n",
    "    tfidf_pipe,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.783\n",
      "\tStandard Deviation: 0.003\n",
      "ROC AUC:\n",
      "\tAverage: 0.874\n",
      "\tStandard Deviation: 0.005\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words(\"english\")\n",
    "nltk_stopwords_tfidf = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer(stop_words=english_stopwords)),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Tfidf NLTK English stopwords random forest\",\n",
    "    nltk_stopwords_tfidf,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.767\n",
      "\tStandard Deviation: 0.006\n",
      "ROC AUC:\n",
      "\tAverage: 0.869\n",
      "\tStandard Deviation: 0.009\n"
     ]
    }
   ],
   "source": [
    "sklearn_stopwords_tfidf = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Tfidf Sklearn English stopwords random forest\",\n",
    "    sklearn_stopwords_tfidf,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>ROC_AUC_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,6) logistic</th>\n",
       "      <td>0.837519</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.923966</td>\n",
       "      <td>0.008160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized TF-IDF logistic. C=0.8</th>\n",
       "      <td>0.836734</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.922593</td>\n",
       "      <td>0.008359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,5) logistic</th>\n",
       "      <td>0.836348</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.922453</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.897232</td>\n",
       "      <td>0.009240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords logistic</th>\n",
       "      <td>0.812407</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF Logistic baseline</th>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.894245</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords logistic</th>\n",
       "      <td>0.804168</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.887977</td>\n",
       "      <td>0.005197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords logistic</th>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords logistic</th>\n",
       "      <td>0.791995</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.878878</td>\n",
       "      <td>0.004080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing NLTK English stopwords logistic</th>\n",
       "      <td>0.782572</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.870553</td>\n",
       "      <td>0.006265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Sklearn English stopwords logistic</th>\n",
       "      <td>0.774733</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.865521</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Logistic baseline</th>\n",
       "      <td>0.772374</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.865579</td>\n",
       "      <td>0.006791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Random Forest</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords random forest</th>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.872445</td>\n",
       "      <td>0.019879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords random forest</th>\n",
       "      <td>0.786492</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.874832</td>\n",
       "      <td>0.005678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,8) random forest</th>\n",
       "      <td>0.817899</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.883957</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF random forest baseline</th>\n",
       "      <td>0.782180</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.875843</td>\n",
       "      <td>0.005262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords random forest</th>\n",
       "      <td>0.783360</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.873547</td>\n",
       "      <td>0.004801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords random forest</th>\n",
       "      <td>0.766881</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>0.869355</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,2) random forest</th>\n",
       "      <td>0.773177</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>0.867305</td>\n",
       "      <td>0.012910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,3) random forest</th>\n",
       "      <td>0.788859</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>0.010972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,4) random forest</th>\n",
       "      <td>0.779833</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>0.864134</td>\n",
       "      <td>0.008181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,5) random forest</th>\n",
       "      <td>0.770411</td>\n",
       "      <td>0.016008</td>\n",
       "      <td>0.857469</td>\n",
       "      <td>0.014217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,6) random forest</th>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.014826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,7) random forest</th>\n",
       "      <td>0.784919</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.866738</td>\n",
       "      <td>0.006657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,8) random forest</th>\n",
       "      <td>0.777863</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.857660</td>\n",
       "      <td>0.007620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,9) random forest</th>\n",
       "      <td>0.788473</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.874884</td>\n",
       "      <td>0.013938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,10) random forest</th>\n",
       "      <td>0.779845</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.869789</td>\n",
       "      <td>0.012192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,11) random forest</th>\n",
       "      <td>0.790819</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.872329</td>\n",
       "      <td>0.004028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,12) random forest</th>\n",
       "      <td>0.777473</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.855719</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,13) random forest</th>\n",
       "      <td>0.767256</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.859222</td>\n",
       "      <td>0.005307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,14) random forest</th>\n",
       "      <td>0.783355</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>0.860557</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,15) random forest</th>\n",
       "      <td>0.791608</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.008224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,16) random forest</th>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.014033</td>\n",
       "      <td>0.866515</td>\n",
       "      <td>0.011130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,17) random forest</th>\n",
       "      <td>0.784542</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.869532</td>\n",
       "      <td>0.009101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,18) random forest</th>\n",
       "      <td>0.777866</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.857273</td>\n",
       "      <td>0.005854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,19) random forest</th>\n",
       "      <td>0.770819</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.854933</td>\n",
       "      <td>0.018063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,20) random forest</th>\n",
       "      <td>0.782583</td>\n",
       "      <td>0.011301</td>\n",
       "      <td>0.865904</td>\n",
       "      <td>0.016258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,21) random forest</th>\n",
       "      <td>0.788082</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.859405</td>\n",
       "      <td>0.006646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,22) random forest</th>\n",
       "      <td>0.780238</td>\n",
       "      <td>0.016206</td>\n",
       "      <td>0.858827</td>\n",
       "      <td>0.014143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,23) random forest</th>\n",
       "      <td>0.782175</td>\n",
       "      <td>0.006093</td>\n",
       "      <td>0.862112</td>\n",
       "      <td>0.006244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,24) random forest</th>\n",
       "      <td>0.794751</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.863702</td>\n",
       "      <td>0.011898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,25) random forest</th>\n",
       "      <td>0.785335</td>\n",
       "      <td>0.020483</td>\n",
       "      <td>0.869313</td>\n",
       "      <td>0.008894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,26) random forest</th>\n",
       "      <td>0.781802</td>\n",
       "      <td>0.013310</td>\n",
       "      <td>0.859437</td>\n",
       "      <td>0.009807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,27) random forest</th>\n",
       "      <td>0.782967</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.860471</td>\n",
       "      <td>0.003305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,28) random forest</th>\n",
       "      <td>0.784536</td>\n",
       "      <td>0.017803</td>\n",
       "      <td>0.866469</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,29) random forest</th>\n",
       "      <td>0.784548</td>\n",
       "      <td>0.010176</td>\n",
       "      <td>0.866170</td>\n",
       "      <td>0.010098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  Accuracy_std  \\\n",
       "Tfidf N-grams (1,6) logistic                   0.837519      0.000181   \n",
       "Regularized TF-IDF logistic. C=0.8             0.836734      0.000664   \n",
       "Count N-grams (1,5) logistic                   0.836348      0.009070   \n",
       "Count Logistic                                 0.817895      0.010648   \n",
       "Count NLTK English stopwords logistic          0.812407      0.008775   \n",
       "TF-IDF Logistic baseline                       0.805341      0.008842   \n",
       "Count Sklearn English stopwords logistic       0.804168      0.009856   \n",
       "Tfidf NLTK English stopwords logistic          0.801015      0.008513   \n",
       "Tfidf Sklearn English stopwords logistic       0.791995      0.002666   \n",
       "Hashing NLTK English stopwords logistic        0.782572      0.008452   \n",
       "Hashing Sklearn English stopwords logistic     0.774733      0.008527   \n",
       "Hashing Logistic baseline                      0.772374      0.007089   \n",
       "Count Logistic                                 0.799059      0.002669   \n",
       "Count Random Forest                            0.799059      0.002669   \n",
       "Count NLTK English stopwords random forest     0.779434      0.011555   \n",
       "Count Sklearn English stopwords random forest  0.786492      0.009104   \n",
       "Count N-grams (1,8) random forest              0.817899      0.003519   \n",
       "TF-IDF random forest baseline                  0.782180      0.005168   \n",
       "Tfidf NLTK English stopwords random forest     0.783360      0.003414   \n",
       "Tfidf Sklearn English stopwords random forest  0.766881      0.005547   \n",
       "Tfidf N-grams (1,2) random forest              0.773177      0.022872   \n",
       "Tfidf N-grams (1,3) random forest              0.788859      0.008762   \n",
       "Tfidf N-grams (1,4) random forest              0.779833      0.009866   \n",
       "Tfidf N-grams (1,5) random forest              0.770411      0.016008   \n",
       "Tfidf N-grams (1,6) random forest              0.767677      0.014314   \n",
       "Tfidf N-grams (1,7) random forest              0.784919      0.015054   \n",
       "Tfidf N-grams (1,8) random forest              0.777863      0.017905   \n",
       "Tfidf N-grams (1,9) random forest              0.788473      0.010714   \n",
       "Tfidf N-grams (1,10) random forest             0.779845      0.015594   \n",
       "Tfidf N-grams (1,11) random forest             0.790819      0.011402   \n",
       "Tfidf N-grams (1,12) random forest             0.777473      0.004362   \n",
       "Tfidf N-grams (1,13) random forest             0.767256      0.011529   \n",
       "Tfidf N-grams (1,14) random forest             0.783355      0.004636   \n",
       "Tfidf N-grams (1,15) random forest             0.791608      0.010777   \n",
       "Tfidf N-grams (1,16) random forest             0.786900      0.014033   \n",
       "Tfidf N-grams (1,17) random forest             0.784542      0.005564   \n",
       "Tfidf N-grams (1,18) random forest             0.777866      0.004471   \n",
       "Tfidf N-grams (1,19) random forest             0.770819      0.016196   \n",
       "Tfidf N-grams (1,20) random forest             0.782583      0.011301   \n",
       "Tfidf N-grams (1,21) random forest             0.788082      0.014930   \n",
       "Tfidf N-grams (1,22) random forest             0.780238      0.016206   \n",
       "Tfidf N-grams (1,23) random forest             0.782175      0.006093   \n",
       "Tfidf N-grams (1,24) random forest             0.794751      0.015519   \n",
       "Tfidf N-grams (1,25) random forest             0.785335      0.020483   \n",
       "Tfidf N-grams (1,26) random forest             0.781802      0.013310   \n",
       "Tfidf N-grams (1,27) random forest             0.782967      0.001441   \n",
       "Tfidf N-grams (1,28) random forest             0.784536      0.017803   \n",
       "Tfidf N-grams (1,29) random forest             0.784548      0.010176   \n",
       "\n",
       "                                                ROC_AUC  ROC_AUC_std  \n",
       "Tfidf N-grams (1,6) logistic                   0.923966     0.008160  \n",
       "Regularized TF-IDF logistic. C=0.8             0.922593     0.008359  \n",
       "Count N-grams (1,5) logistic                   0.922453     0.006407  \n",
       "Count Logistic                                 0.897232     0.009240  \n",
       "Count NLTK English stopwords logistic          0.891127     0.005639  \n",
       "TF-IDF Logistic baseline                       0.894245     0.005339  \n",
       "Count Sklearn English stopwords logistic       0.887977     0.005197  \n",
       "Tfidf NLTK English stopwords logistic          0.884795     0.005838  \n",
       "Tfidf Sklearn English stopwords logistic       0.878878     0.004080  \n",
       "Hashing NLTK English stopwords logistic        0.870553     0.006265  \n",
       "Hashing Sklearn English stopwords logistic     0.865521     0.005346  \n",
       "Hashing Logistic baseline                      0.865579     0.006791  \n",
       "Count Logistic                                 0.884808     0.006048  \n",
       "Count Random Forest                            0.884808     0.006048  \n",
       "Count NLTK English stopwords random forest     0.872445     0.019879  \n",
       "Count Sklearn English stopwords random forest  0.874832     0.005678  \n",
       "Count N-grams (1,8) random forest              0.883957     0.004047  \n",
       "TF-IDF random forest baseline                  0.875843     0.005262  \n",
       "Tfidf NLTK English stopwords random forest     0.873547     0.004801  \n",
       "Tfidf Sklearn English stopwords random forest  0.869355     0.008900  \n",
       "Tfidf N-grams (1,2) random forest              0.867305     0.012910  \n",
       "Tfidf N-grams (1,3) random forest              0.875300     0.010972  \n",
       "Tfidf N-grams (1,4) random forest              0.864134     0.008181  \n",
       "Tfidf N-grams (1,5) random forest              0.857469     0.014217  \n",
       "Tfidf N-grams (1,6) random forest              0.856400     0.014826  \n",
       "Tfidf N-grams (1,7) random forest              0.866738     0.006657  \n",
       "Tfidf N-grams (1,8) random forest              0.857660     0.007620  \n",
       "Tfidf N-grams (1,9) random forest              0.874884     0.013938  \n",
       "Tfidf N-grams (1,10) random forest             0.869789     0.012192  \n",
       "Tfidf N-grams (1,11) random forest             0.872329     0.004028  \n",
       "Tfidf N-grams (1,12) random forest             0.855719     0.009283  \n",
       "Tfidf N-grams (1,13) random forest             0.859222     0.005307  \n",
       "Tfidf N-grams (1,14) random forest             0.860557     0.000745  \n",
       "Tfidf N-grams (1,15) random forest             0.863082     0.008224  \n",
       "Tfidf N-grams (1,16) random forest             0.866515     0.011130  \n",
       "Tfidf N-grams (1,17) random forest             0.869532     0.009101  \n",
       "Tfidf N-grams (1,18) random forest             0.857273     0.005854  \n",
       "Tfidf N-grams (1,19) random forest             0.854933     0.018063  \n",
       "Tfidf N-grams (1,20) random forest             0.865904     0.016258  \n",
       "Tfidf N-grams (1,21) random forest             0.859405     0.006646  \n",
       "Tfidf N-grams (1,22) random forest             0.858827     0.014143  \n",
       "Tfidf N-grams (1,23) random forest             0.862112     0.006244  \n",
       "Tfidf N-grams (1,24) random forest             0.863702     0.011898  \n",
       "Tfidf N-grams (1,25) random forest             0.869313     0.008894  \n",
       "Tfidf N-grams (1,26) random forest             0.859437     0.009807  \n",
       "Tfidf N-grams (1,27) random forest             0.860471     0.003305  \n",
       "Tfidf N-grams (1,28) random forest             0.866469     0.014327  \n",
       "Tfidf N-grams (1,29) random forest             0.866170     0.010098  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ngram = results\n",
    "for max_ngram in range(2, 30):\n",
    "    sklearn_tfidf_ngram = Pipeline([\n",
    "        (\"vectorize\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1, max_ngram))),\n",
    "        (\"model\", RandomForestClassifier(random_state=seed))\n",
    "    ])\n",
    "    results_ngram = score_model_oversampling(\n",
    "        \"Tfidf N-grams (1,{0}) random forest\".format(max_ngram),\n",
    "        sklearn_tfidf_ngram,\n",
    "        results_ngram,\n",
    "        result_columns,\n",
    "        False\n",
    "    )\n",
    "results_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>ROC_AUC_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,6) logistic</th>\n",
       "      <td>0.837519</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.923966</td>\n",
       "      <td>0.008160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized TF-IDF logistic. C=0.8</th>\n",
       "      <td>0.836734</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.922593</td>\n",
       "      <td>0.008359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,5) logistic</th>\n",
       "      <td>0.836348</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.922453</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.897232</td>\n",
       "      <td>0.009240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords logistic</th>\n",
       "      <td>0.812407</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF Logistic baseline</th>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.894245</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords logistic</th>\n",
       "      <td>0.804168</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.887977</td>\n",
       "      <td>0.005197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords logistic</th>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords logistic</th>\n",
       "      <td>0.791995</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.878878</td>\n",
       "      <td>0.004080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing NLTK English stopwords logistic</th>\n",
       "      <td>0.782572</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.870553</td>\n",
       "      <td>0.006265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Sklearn English stopwords logistic</th>\n",
       "      <td>0.774733</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.865521</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Logistic baseline</th>\n",
       "      <td>0.772374</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.865579</td>\n",
       "      <td>0.006791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Random Forest</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords random forest</th>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.872445</td>\n",
       "      <td>0.019879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords random forest</th>\n",
       "      <td>0.786492</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.874832</td>\n",
       "      <td>0.005678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,8) random forest</th>\n",
       "      <td>0.817899</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.883957</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF random forest baseline</th>\n",
       "      <td>0.782180</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.875843</td>\n",
       "      <td>0.005262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords random forest</th>\n",
       "      <td>0.783360</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.873547</td>\n",
       "      <td>0.004801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords random forest</th>\n",
       "      <td>0.766881</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>0.869355</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (2, 11) random forest</th>\n",
       "      <td>0.783356</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.833038</td>\n",
       "      <td>0.002101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (3, 11) random forest</th>\n",
       "      <td>0.785324</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.838178</td>\n",
       "      <td>0.004422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (4, 11) random forest</th>\n",
       "      <td>0.778664</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.805537</td>\n",
       "      <td>0.007441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (5, 11) random forest</th>\n",
       "      <td>0.772384</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.781924</td>\n",
       "      <td>0.012081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (6, 11) random forest</th>\n",
       "      <td>0.770028</td>\n",
       "      <td>0.011023</td>\n",
       "      <td>0.770960</td>\n",
       "      <td>0.010821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (7, 11) random forest</th>\n",
       "      <td>0.764532</td>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.765117</td>\n",
       "      <td>0.010296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (8, 11) random forest</th>\n",
       "      <td>0.755896</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.755896</td>\n",
       "      <td>0.008462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (9, 11) random forest</th>\n",
       "      <td>0.742945</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.742945</td>\n",
       "      <td>0.008664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (10, 11) random forest</th>\n",
       "      <td>0.730776</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.730776</td>\n",
       "      <td>0.006257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  Accuracy_std  \\\n",
       "Tfidf N-grams (1,6) logistic                   0.837519      0.000181   \n",
       "Regularized TF-IDF logistic. C=0.8             0.836734      0.000664   \n",
       "Count N-grams (1,5) logistic                   0.836348      0.009070   \n",
       "Count Logistic                                 0.817895      0.010648   \n",
       "Count NLTK English stopwords logistic          0.812407      0.008775   \n",
       "TF-IDF Logistic baseline                       0.805341      0.008842   \n",
       "Count Sklearn English stopwords logistic       0.804168      0.009856   \n",
       "Tfidf NLTK English stopwords logistic          0.801015      0.008513   \n",
       "Tfidf Sklearn English stopwords logistic       0.791995      0.002666   \n",
       "Hashing NLTK English stopwords logistic        0.782572      0.008452   \n",
       "Hashing Sklearn English stopwords logistic     0.774733      0.008527   \n",
       "Hashing Logistic baseline                      0.772374      0.007089   \n",
       "Count Logistic                                 0.799059      0.002669   \n",
       "Count Random Forest                            0.799059      0.002669   \n",
       "Count NLTK English stopwords random forest     0.779434      0.011555   \n",
       "Count Sklearn English stopwords random forest  0.786492      0.009104   \n",
       "Count N-grams (1,8) random forest              0.817899      0.003519   \n",
       "TF-IDF random forest baseline                  0.782180      0.005168   \n",
       "Tfidf NLTK English stopwords random forest     0.783360      0.003414   \n",
       "Tfidf Sklearn English stopwords random forest  0.766881      0.005547   \n",
       "Tfidf N-grams (2, 11) random forest            0.783356      0.005247   \n",
       "Tfidf N-grams (3, 11) random forest            0.785324      0.001772   \n",
       "Tfidf N-grams (4, 11) random forest            0.778664      0.012261   \n",
       "Tfidf N-grams (5, 11) random forest            0.772384      0.012560   \n",
       "Tfidf N-grams (6, 11) random forest            0.770028      0.011023   \n",
       "Tfidf N-grams (7, 11) random forest            0.764532      0.009925   \n",
       "Tfidf N-grams (8, 11) random forest            0.755896      0.008462   \n",
       "Tfidf N-grams (9, 11) random forest            0.742945      0.008664   \n",
       "Tfidf N-grams (10, 11) random forest           0.730776      0.006257   \n",
       "\n",
       "                                                ROC_AUC  ROC_AUC_std  \n",
       "Tfidf N-grams (1,6) logistic                   0.923966     0.008160  \n",
       "Regularized TF-IDF logistic. C=0.8             0.922593     0.008359  \n",
       "Count N-grams (1,5) logistic                   0.922453     0.006407  \n",
       "Count Logistic                                 0.897232     0.009240  \n",
       "Count NLTK English stopwords logistic          0.891127     0.005639  \n",
       "TF-IDF Logistic baseline                       0.894245     0.005339  \n",
       "Count Sklearn English stopwords logistic       0.887977     0.005197  \n",
       "Tfidf NLTK English stopwords logistic          0.884795     0.005838  \n",
       "Tfidf Sklearn English stopwords logistic       0.878878     0.004080  \n",
       "Hashing NLTK English stopwords logistic        0.870553     0.006265  \n",
       "Hashing Sklearn English stopwords logistic     0.865521     0.005346  \n",
       "Hashing Logistic baseline                      0.865579     0.006791  \n",
       "Count Logistic                                 0.884808     0.006048  \n",
       "Count Random Forest                            0.884808     0.006048  \n",
       "Count NLTK English stopwords random forest     0.872445     0.019879  \n",
       "Count Sklearn English stopwords random forest  0.874832     0.005678  \n",
       "Count N-grams (1,8) random forest              0.883957     0.004047  \n",
       "TF-IDF random forest baseline                  0.875843     0.005262  \n",
       "Tfidf NLTK English stopwords random forest     0.873547     0.004801  \n",
       "Tfidf Sklearn English stopwords random forest  0.869355     0.008900  \n",
       "Tfidf N-grams (2, 11) random forest            0.833038     0.002101  \n",
       "Tfidf N-grams (3, 11) random forest            0.838178     0.004422  \n",
       "Tfidf N-grams (4, 11) random forest            0.805537     0.007441  \n",
       "Tfidf N-grams (5, 11) random forest            0.781924     0.012081  \n",
       "Tfidf N-grams (6, 11) random forest            0.770960     0.010821  \n",
       "Tfidf N-grams (7, 11) random forest            0.765117     0.010296  \n",
       "Tfidf N-grams (8, 11) random forest            0.755896     0.008462  \n",
       "Tfidf N-grams (9, 11) random forest            0.742945     0.008664  \n",
       "Tfidf N-grams (10, 11) random forest           0.730776     0.006257  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ngram = results\n",
    "for min_ngram in range(2, 11):\n",
    "    sklearn_tfidf_ngram = Pipeline([\n",
    "        (\"vectorize\", TfidfVectorizer(analyzer=\"word\", ngram_range=(min_ngram, 11))),\n",
    "        (\"model\", RandomForestClassifier(random_state=seed))\n",
    "    ])\n",
    "    results_ngram = score_model_oversampling(\n",
    "        \"Tfidf N-grams ({0}, 11) random forest\".format(min_ngram),\n",
    "        sklearn_tfidf_ngram,\n",
    "        results_ngram,\n",
    "        result_columns,\n",
    "        False\n",
    "    )\n",
    "results_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.791\n",
      "\tStandard Deviation: 0.011\n",
      "ROC AUC:\n",
      "\tAverage: 0.872\n",
      "\tStandard Deviation: 0.004\n"
     ]
    }
   ],
   "source": [
    "sklearn_tfidf_ngram_1_11 = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 11))),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Tfidf N-grams (1,11) random forest\",\n",
    "    sklearn_tfidf_ngram_1_11,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(\n",
    "    lambda: TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 11)),\n",
    "    lambda: RandomForestClassifier(random_state=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = RandomForestClassifier(random_state=seed, C=coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-589d5653b102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     regularized_tfidf = Pipeline([\n\u001b[0;32m      5\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"vectorize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     ])\n\u001b[0;32m      8\u001b[0m     coef_results = score_model_oversampling(\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'C'"
     ]
    }
   ],
   "source": [
    "coef_results = results\n",
    "for coef_part in range(1, 20):\n",
    "    coef = float(coef_part)/10\n",
    "    regularized_tfidf = Pipeline([\n",
    "        (\"vectorize\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 6))),\n",
    "        (\"model\", RandomForestClassifier(random_state=seed, C=coef))\n",
    "    ])\n",
    "    coef_results = score_model_oversampling(\n",
    "        \"Regularized TF-IDF random forest. C={0}\".format(coef),\n",
    "        regularized_tfidf,\n",
    "        coef_results,\n",
    "        result_columns,\n",
    "        False\n",
    "    )\n",
    "coef_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.837\n",
      "\tStandard Deviation: 0.001\n",
      "ROC AUC:\n",
      "\tAverage: 0.923\n",
      "\tStandard Deviation: 0.008\n"
     ]
    }
   ],
   "source": [
    "regularized_tfidf_c_0_8 = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 6))),\n",
    "    (\"model\", LogisticRegression(random_state=seed, C=0.8))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Regularized TF-IDF logistic. C=0.8\",\n",
    "    regularized_tfidf_c_0_8,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main features: ['great', 'not', 'and', 'only', 'good']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30825150859173023"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_metrics(\n",
    "    lambda: TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 6)),\n",
    "    lambda: LogisticRegression(random_state=seed, C=1.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.781\n",
      "\tStandard Deviation: 0.013\n",
      "ROC AUC:\n",
      "\tAverage: 0.872\n",
      "\tStandard Deviation: 0.009\n"
     ]
    }
   ],
   "source": [
    "hashing_pipe = Pipeline([\n",
    "    (\"vectorize\", HashingVectorizer()),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))])\n",
    "results = score_model_oversampling(\n",
    "    \"Hashing random forest baseline\",\n",
    "    hashing_pipe,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.777\n",
      "\tStandard Deviation: 0.016\n",
      "ROC AUC:\n",
      "\tAverage: 0.869\n",
      "\tStandard Deviation: 0.009\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words(\"english\")\n",
    "nltk_stopwords_hashing = Pipeline([\n",
    "    (\"vectorize\", HashingVectorizer(stop_words=english_stopwords)),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Hashing NLTK English stopwords random forest\",\n",
    "    nltk_stopwords_hashing,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\tAverage: 0.782\n",
      "\tStandard Deviation: 0.026\n",
      "ROC AUC:\n",
      "\tAverage: 0.866\n",
      "\tStandard Deviation: 0.019\n"
     ]
    }
   ],
   "source": [
    "sklearn_stopwords_hashing = Pipeline([\n",
    "    (\"vectorize\", HashingVectorizer(stop_words=\"english\")),\n",
    "    (\"model\", RandomForestClassifier(random_state=seed))\n",
    "])\n",
    "results = score_model_oversampling(\n",
    "    \"Hashing Sklearn English stopwords random forest\",\n",
    "    sklearn_stopwords_hashing,\n",
    "    results,\n",
    "    result_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>ROC_AUC_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,6) logistic</th>\n",
       "      <td>0.837519</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.923966</td>\n",
       "      <td>0.008160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized TF-IDF logistic. C=0.8</th>\n",
       "      <td>0.836734</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.922593</td>\n",
       "      <td>0.008359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,5) logistic</th>\n",
       "      <td>0.836348</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.922453</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count N-grams (1,8) random forest</th>\n",
       "      <td>0.817899</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.883957</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.897232</td>\n",
       "      <td>0.009240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords logistic</th>\n",
       "      <td>0.812407</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF Logistic baseline</th>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.894245</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords logistic</th>\n",
       "      <td>0.804168</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.887977</td>\n",
       "      <td>0.005197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords logistic</th>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Random Forest</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Logistic</th>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.884808</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords logistic</th>\n",
       "      <td>0.791995</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.878878</td>\n",
       "      <td>0.004080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf N-grams (1,11) random forest</th>\n",
       "      <td>0.790819</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.872329</td>\n",
       "      <td>0.004028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Sklearn English stopwords random forest</th>\n",
       "      <td>0.786492</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.874832</td>\n",
       "      <td>0.005678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf NLTK English stopwords random forest</th>\n",
       "      <td>0.783360</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.873547</td>\n",
       "      <td>0.004801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing NLTK English stopwords logistic</th>\n",
       "      <td>0.782572</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.870553</td>\n",
       "      <td>0.006265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF random forest baseline</th>\n",
       "      <td>0.782180</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.875843</td>\n",
       "      <td>0.005262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Sklearn English stopwords random forest</th>\n",
       "      <td>0.781773</td>\n",
       "      <td>0.025503</td>\n",
       "      <td>0.866172</td>\n",
       "      <td>0.018525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing random forest baseline</th>\n",
       "      <td>0.781405</td>\n",
       "      <td>0.012663</td>\n",
       "      <td>0.871920</td>\n",
       "      <td>0.009414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count NLTK English stopwords random forest</th>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.872445</td>\n",
       "      <td>0.019879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing NLTK English stopwords random forest</th>\n",
       "      <td>0.777490</td>\n",
       "      <td>0.016455</td>\n",
       "      <td>0.869408</td>\n",
       "      <td>0.009320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Sklearn English stopwords logistic</th>\n",
       "      <td>0.774733</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.865521</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashing Logistic baseline</th>\n",
       "      <td>0.772374</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.865579</td>\n",
       "      <td>0.006791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf Sklearn English stopwords random forest</th>\n",
       "      <td>0.766881</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>0.869355</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Accuracy  Accuracy_std  \\\n",
       "Tfidf N-grams (1,6) logistic                     0.837519      0.000181   \n",
       "Regularized TF-IDF logistic. C=0.8               0.836734      0.000664   \n",
       "Count N-grams (1,5) logistic                     0.836348      0.009070   \n",
       "Count N-grams (1,8) random forest                0.817899      0.003519   \n",
       "Count Logistic                                   0.817895      0.010648   \n",
       "Count NLTK English stopwords logistic            0.812407      0.008775   \n",
       "TF-IDF Logistic baseline                         0.805341      0.008842   \n",
       "Count Sklearn English stopwords logistic         0.804168      0.009856   \n",
       "Tfidf NLTK English stopwords logistic            0.801015      0.008513   \n",
       "Count Random Forest                              0.799059      0.002669   \n",
       "Count Logistic                                   0.799059      0.002669   \n",
       "Tfidf Sklearn English stopwords logistic         0.791995      0.002666   \n",
       "Tfidf N-grams (1,11) random forest               0.790819      0.011402   \n",
       "Count Sklearn English stopwords random forest    0.786492      0.009104   \n",
       "Tfidf NLTK English stopwords random forest       0.783360      0.003414   \n",
       "Hashing NLTK English stopwords logistic          0.782572      0.008452   \n",
       "TF-IDF random forest baseline                    0.782180      0.005168   \n",
       "Hashing Sklearn English stopwords random forest  0.781773      0.025503   \n",
       "Hashing random forest baseline                   0.781405      0.012663   \n",
       "Count NLTK English stopwords random forest       0.779434      0.011555   \n",
       "Hashing NLTK English stopwords random forest     0.777490      0.016455   \n",
       "Hashing Sklearn English stopwords logistic       0.774733      0.008527   \n",
       "Hashing Logistic baseline                        0.772374      0.007089   \n",
       "Tfidf Sklearn English stopwords random forest    0.766881      0.005547   \n",
       "\n",
       "                                                  ROC_AUC  ROC_AUC_std  \n",
       "Tfidf N-grams (1,6) logistic                     0.923966     0.008160  \n",
       "Regularized TF-IDF logistic. C=0.8               0.922593     0.008359  \n",
       "Count N-grams (1,5) logistic                     0.922453     0.006407  \n",
       "Count N-grams (1,8) random forest                0.883957     0.004047  \n",
       "Count Logistic                                   0.897232     0.009240  \n",
       "Count NLTK English stopwords logistic            0.891127     0.005639  \n",
       "TF-IDF Logistic baseline                         0.894245     0.005339  \n",
       "Count Sklearn English stopwords logistic         0.887977     0.005197  \n",
       "Tfidf NLTK English stopwords logistic            0.884795     0.005838  \n",
       "Count Random Forest                              0.884808     0.006048  \n",
       "Count Logistic                                   0.884808     0.006048  \n",
       "Tfidf Sklearn English stopwords logistic         0.878878     0.004080  \n",
       "Tfidf N-grams (1,11) random forest               0.872329     0.004028  \n",
       "Count Sklearn English stopwords random forest    0.874832     0.005678  \n",
       "Tfidf NLTK English stopwords random forest       0.873547     0.004801  \n",
       "Hashing NLTK English stopwords logistic          0.870553     0.006265  \n",
       "TF-IDF random forest baseline                    0.875843     0.005262  \n",
       "Hashing Sklearn English stopwords random forest  0.866172     0.018525  \n",
       "Hashing random forest baseline                   0.871920     0.009414  \n",
       "Count NLTK English stopwords random forest       0.872445     0.019879  \n",
       "Hashing NLTK English stopwords random forest     0.869408     0.009320  \n",
       "Hashing Sklearn English stopwords logistic       0.865521     0.005346  \n",
       "Hashing Logistic baseline                        0.865579     0.006791  \n",
       "Tfidf Sklearn English stopwords random forest    0.869355     0.008900  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.sort_values(\"Accuracy\", ascending=False)\n",
    "results.to_csv(\"..\\..\\Results\\sentiment_kaggle_results_table.csv\", index=True, index_label=\"Id\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  1\n",
       "4  0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = test_data[\"text\"].tolist()\n",
    "balanced_texts, balanced_classes = balance_data(texts, classes)\n",
    "model = regularized_tfidf_c_0_8.fit(balanced_texts, balanced_classes)\n",
    "result_frame = pd.DataFrame(model.predict(test_texts), columns=[\"y\"])\n",
    "result_frame.to_csv(\"..\\\\..\\\\Results\\\\balanced_tfidf_ng1_6_c_0_8_logistic.csv\", index=True, index_label=\"Id\")\n",
    "result_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
