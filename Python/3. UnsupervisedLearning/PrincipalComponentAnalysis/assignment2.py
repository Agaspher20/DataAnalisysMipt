## Метод главных компонент
# В данном задании вам будет предложено ознакомиться с подходом, который переоткрывался в самых разных областях,
# имеет множество разных интерпретаций, а также несколько интересных обобщений: методом главных компонент
# (principal component analysis).

## Programming assignment
# Задание разбито на две части: 
# работа с модельными данными,
# работа с реальными данными.
# В конце каждого пункта от вас требуется получить ответ и загрузить в соответствующую форму в виде набора
# текстовых файлов.
#%%
import numpy as np
import pandas as pd
import matplotlib
from matplotlib import pyplot as plt
import matplotlib.patches as mpatches
matplotlib.style.use("ggplot")
%matplotlib inline

from sklearn.decomposition import PCA

## Вариационный взгляд на модель
# Мы знаем, что каждой главной компоненте соответствует описываемая ей дисперсия данных (дисперсия данных при
# проекции на эту компоненту). Она численно равна значению диагональных элементов матрицы Λ, получаемой из
# спектрального разложения матрицы ковариации данных (смотри теорию выше).
# Исходя из этого, мы можем отсортировать дисперсию данных вдоль этих компонент по убыванию, и уменьшить
# размерность данных, отбросив q итоговых главных компонент, имеющих наименьшую дисперсию.
# Делать это можно двумя разными способами. Например, если вы вдальнейшем обучаете на данных с уменьшенной
# размерностью модель классификации или регрессии, то можно запустить итерационный процесс: удалять компоненты с
# наименьшей дисперсией по одной, пока качество итоговой модели не станет значительно хуже.
# Более общий способ отбора признаков заключается в том, что вы можете посмотреть на разности в дисперсиях в
# отсортированном ряде λ(1) > λ(2) >⋯> λ(D): λ(1) − λ(2),…,λ(D−1) − λ(D), и удалить те компоненты, на которых
# разность будет наибольшей. Именно этим методом вам и предлагается воспользоваться для тестового набора данных.

## Задание 2. Ручное уменьшение размерности признаков посредством анализа дисперсии данных вдоль главных
## компонент
# Рассмотрим ещё один набор данных размерности D, чья реальная размерность значительно меньше наблюдаемой
# (назовём её также d). От вас требуется:
# 1. Построить модель PCA с D главными компонентами по этим данным.
# 2. Спроецировать данные на главные компоненты.
# 3. Оценить их дисперсию вдоль главных компонент.
# 4. Отсортировать дисперсии в порядке убывания и получить их попарные разности: λ(i−1) − λ(i).
# 5. Найти разность с наибольшим значением и получить по ней оценку на эффективную размерность данных d^.
# 6. Построить график дисперсий и убедиться, что полученная оценка на d^opt действительно имеет смысл, после
#    этого внести полученное значение d^opt в файл ответа.
# Для построения модели PCA используйте функцию:
#   model.fit(data)
# Для трансформации данных используйте метод:
#   model.transform(data)
# Оценку дисперсий на трансформированных данных от вас потребуется реализовать вручную. Для построения графиков
# можно воспользоваться функцией
# plot_variances(d_variances)
# которой следует передать на вход отсортированный по убыванию вектор дисперсий вдоль компонент.
#%%
def plot_variances(d_variances):
    n_components = np.arange(1,d_variances.size+1)
    plt.plot(n_components, d_variances, 'b', label='Component variances')
    plt.xlim(n_components[0], n_components[-1])
    plt.xlabel('n components')
    plt.ylabel('variance')
    plt.legend(loc='upper right')
    plt.show()
    
def write_answer_2(optimal_d):
    with open("..\..\..\Results\pca_answer2.txt", "w") as fout:
        fout.write(str(optimal_d))
        
data = pd.read_csv('..\..\..\Data\pca_data_task2.csv')
#%%
D = len(data.columns)
model = PCA(n_components=D)
model.fit(data)
projected_data = model.transform(data)
projected_data_t = projected_data.transpose()

deviations = np.std(projected_data, axis=0)
deviations_indices = [(i,x) for i,x in enumerate(deviations)]
sorted_deviations = sorted(deviations_indices, key=lambda di: -di[1])
differences = [(sorted_deviations[i][0],(sorted_deviations[i-1][1] - sorted_deviations[i][1]))
                for i in xrange(1, D)]
#%%
plot_variances(np.array(map(lambda sd: sd[1], sorted_deviations)))
answer_2 = sorted(differences, key=lambda dif: -dif[1])[0][0]
write_answer_2(answer_2)
answer_2
